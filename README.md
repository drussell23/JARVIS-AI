# ü§ñ JARVIS - Claude-Powered Iron Man AI Agent (v5.8)

<p align="center">
  <img src="https://img.shields.io/badge/AI%20Agent-100%25%20Claude%20Powered-purple" alt="Claude AI">
  <img src="https://img.shields.io/badge/AI-Claude%20Opus%204-blue" alt="Claude AI">
  <img src="https://img.shields.io/badge/Vision-Zero%20Hardcoding%20Dynamic%20ML-orange" alt="Vision System">
  <img src="https://img.shields.io/badge/Architecture-Plugin%20Based%20Extensible-green" alt="Architecture">
  <img src="https://img.shields.io/badge/UI-Iron%20Man%20Inspired-red" alt="Iron Man UI">
  <img src="https://img.shields.io/badge/Learning-Self%20Improving%20AI-yellow" alt="Machine Learning">
  <img src="https://img.shields.io/badge/Status-FULLY%20AUTONOMOUS-success" alt="Production">
  <img src="https://img.shields.io/badge/Version-5.8-brightgreen" alt="Version">
  <img src="https://img.shields.io/badge/Intent-üß†%20ML%20Classification-purple" alt="Intent Analysis">
  <img src="https://img.shields.io/badge/Routing-üéØ%20Dynamic%20Provider%20Selection-cyan" alt="Routing">
  <img src="https://img.shields.io/badge/Performance-‚ö°%20Self%20Optimizing-ff69b4" alt="Performance">
</p>

<p align="center">
  <em>"JARVIS, sometimes you gotta run before you can walk." - Tony Stark</em>
</p>

## üöÄ What's New in v5.8 - Zero-Hardcoding Dynamic Vision System

### **üß† Revolutionary Zero-Hardcoding Vision Architecture**
JARVIS v5.8 introduces a completely dynamic vision system with **ZERO hardcoded patterns or keywords**. Every vision capability is discovered, learned, and improved through machine learning.

### **Key Innovations:**
- **Dynamic Vision Engine** - ML-based intent classification without any hardcoded patterns
- **Plugin Architecture** - Extensible vision provider system with hot-reload
- **Unified Vision System** - Intelligent routing to best provider based on performance
- **Semantic Understanding** - Uses sentence transformers for true intent matching
- **Continuous Learning** - Learns from every command and improves over time
- **Auto-Discovery** - Finds and registers capabilities at runtime

## üèóÔ∏è JARVIS Vision System Architecture

### **Overview**
The JARVIS vision system is built on a revolutionary zero-hardcoding philosophy. Instead of predefined patterns or keyword matching, it uses machine learning to understand user intent and dynamically route requests to the best available vision provider.

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        User Voice Command                       ‚îÇ
‚îÇ                    "Hey JARVIS, describe my screen"             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Vision Action Handler                        ‚îÇ
‚îÇ              (system_control/vision_action_handler.py)          ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Receives command from Claude Command Interpreter             ‚îÇ
‚îÇ  ‚Ä¢ No hardcoded action mapping                                  ‚îÇ
‚îÇ  ‚Ä¢ Routes to Unified Vision System                              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                             ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Unified Vision System                        ‚îÇ
‚îÇ                 (vision/unified_vision_system.py)               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚Ä¢ Analyzes request using ML                                    ‚îÇ
‚îÇ  ‚Ä¢ Determines best routing strategy                             ‚îÇ
‚îÇ  ‚Ä¢ Manages all vision components                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ                   ‚îÇ                   ‚îÇ
         ‚ñº                   ‚ñº                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Dynamic Vision ‚îÇ ‚îÇ  Vision Plugin  ‚îÇ ‚îÇ Provider-Specific‚îÇ
‚îÇ     Engine      ‚îÇ ‚îÇ     System      ‚îÇ ‚îÇ    Handlers     ‚îÇ
‚îÇ                 ‚îÇ ‚îÇ                 ‚îÇ ‚îÇ                 ‚îÇ
‚îÇ ‚Ä¢ ML Intent     ‚îÇ ‚îÇ ‚Ä¢ Provider      ‚îÇ ‚îÇ ‚Ä¢ Claude Vision ‚îÇ
‚îÇ   Classification‚îÇ ‚îÇ   Registry      ‚îÇ ‚îÇ ‚Ä¢ Screen Capture‚îÇ
‚îÇ ‚Ä¢ Pattern       ‚îÇ ‚îÇ ‚Ä¢ Performance   ‚îÇ ‚îÇ ‚Ä¢ OCR Processing‚îÇ
‚îÇ   Learning      ‚îÇ ‚îÇ   Routing       ‚îÇ ‚îÇ ‚Ä¢ Workspace     ‚îÇ
‚îÇ ‚Ä¢ Semantic      ‚îÇ ‚îÇ ‚Ä¢ Hot-reload    ‚îÇ ‚îÇ   Analysis      ‚îÇ
‚îÇ   Matching      ‚îÇ ‚îÇ   Support       ‚îÇ ‚îÇ                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### **Core Components**

#### 1. **Dynamic Vision Engine** (`vision/dynamic_vision_engine.py`)
The heart of the zero-hardcoding system. This engine:
- **Intent Analysis**: Uses NLP to understand what the user wants
- **Semantic Matching**: Employs sentence transformers for meaning-based matching
- **Pattern Learning**: Builds a database of successful command patterns
- **Capability Discovery**: Automatically finds available vision functions
- **Confidence Scoring**: Multi-factor analysis for routing decisions

```python
# Example: How commands are processed
"describe my screen" ‚Üí Intent: describe, Target: screen ‚Üí Best match: screen_capture + claude_analysis
"check for notifications" ‚Üí Intent: check, Target: notifications ‚Üí Best match: notification_detector
"what am I looking at" ‚Üí Intent: query, Context: visual ‚Üí Best match: claude_vision_analyzer
```

#### 2. **Vision Plugin System** (`vision/vision_plugin_system.py`)
Extensible architecture for vision providers:
- **Auto-Discovery**: Scans for plugins in the `plugins/` directory
- **Dynamic Registration**: Providers register their capabilities
- **Performance Tracking**: Monitors success rates and execution times
- **Intelligent Routing**: Routes to best provider based on historical performance
- **Fallback Chains**: Automatic fallback to alternate providers

#### 3. **Unified Vision System** (`vision/unified_vision_system.py`)
The intelligent orchestrator:
- **Request Analysis**: Determines optimal processing strategy
- **Multi-Provider Fusion**: Combines results from multiple providers
- **Context Management**: Maintains conversation context
- **Learning Integration**: Feeds results back for continuous improvement

### **How It Works: Zero Hardcoding in Action**

#### **Traditional Approach (What We DON'T Do):**
```python
# ‚ùå OLD WAY - Hardcoded patterns
if "describe" in command and "screen" in command:
    return describe_screen()
elif "capture" in command:
    return capture_screen()
# ... hundreds of if-else statements
```

#### **Our ML-Based Approach:**
```python
# ‚úÖ NEW WAY - Dynamic ML routing
intent = analyze_intent(command)  # ML understands user intent
capabilities = discover_capabilities()  # Runtime discovery
best_match = score_capabilities(intent, capabilities)  # ML scoring
result = execute_capability(best_match)  # Dynamic execution
learn_from_result(intent, best_match, result)  # Continuous learning
```

### **Plugin Development**

Creating a custom vision provider is simple:

```python
# vision/plugins/my_custom_provider.py
from vision.vision_plugin_system import BaseVisionProvider

class MyCustomProvider(BaseVisionProvider):
    def _initialize(self):
        # Register what your provider can do
        self.register_capability("custom_analysis", confidence=0.9)
        self.register_capability("special_detection", confidence=0.85)
        
    async def execute(self, capability: str, **kwargs):
        if capability == "custom_analysis":
            # Your custom implementation
            return await self.analyze_custom(**kwargs)
```

Drop this file in `backend/vision/plugins/` and it's automatically discovered!

### **Learning System**

The system continuously improves through:

1. **Success Tracking**: Records which capabilities work best for which intents
2. **Pattern Database**: Stores successful command ‚Üí capability mappings
3. **Confidence Adjustment**: Updates provider confidence based on performance
4. **User Feedback**: Learns from corrections and preferences
5. **Semantic Embeddings**: Builds understanding of command variations

### **Performance Optimizations**

- **Parallel Execution**: Multiple providers can run simultaneously
- **Smart Caching**: Recent results cached for repeated queries
- **Lazy Loading**: Providers loaded only when needed
- **Performance-Based Routing**: Fastest providers preferred
- **Adaptive Timeouts**: Adjusts based on provider history

## üéØ System Design Philosophy

### **Zero Hardcoding Principles**

1. **No Keywords**: The system doesn't look for specific words
2. **No Fixed Patterns**: No regex or string matching
3. **No Predefined Actions**: Actions discovered at runtime
4. **No Static Mappings**: All routing is dynamic
5. **No Manual Updates**: System self-improves

### **Machine Learning Architecture**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ          User Command Input              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         NLP Intent Extraction            ‚îÇ
‚îÇ   ‚Ä¢ Verb/Action identification           ‚îÇ
‚îÇ   ‚Ä¢ Target/Object detection              ‚îÇ
‚îÇ   ‚Ä¢ Modifier extraction                  ‚îÇ
‚îÇ   ‚Ä¢ Context understanding                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Semantic Embedding Generation       ‚îÇ
‚îÇ   ‚Ä¢ Sentence transformers                ‚îÇ
‚îÇ   ‚Ä¢ Contextual embeddings                ‚îÇ
‚îÇ   ‚Ä¢ Similarity scoring                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Capability Scoring & Matching       ‚îÇ
‚îÇ   ‚Ä¢ Semantic similarity                  ‚îÇ
‚îÇ   ‚Ä¢ Historical performance               ‚îÇ
‚îÇ   ‚Ä¢ Context relevance                    ‚îÇ
‚îÇ   ‚Ä¢ Confidence thresholds                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ
                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Dynamic Execution                ‚îÇ
‚îÇ   ‚Ä¢ Provider selection                   ‚îÇ
‚îÇ   ‚Ä¢ Parameter preparation                ‚îÇ
‚îÇ   ‚Ä¢ Result processing                    ‚îÇ
‚îÇ   ‚Ä¢ Learning feedback                    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üöÄ Quick Start

### Prerequisites
- Python 3.8+
- macOS (for full system control features)
- Claude API key from [Anthropic Console](https://console.anthropic.com/)

### Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/JARVIS-AI-Agent.git
cd JARVIS-AI-Agent

# Create .env file in backend/
echo "ANTHROPIC_API_KEY=your-api-key-here" > backend/.env

# Install dependencies
pip install -r backend/requirements.txt

# Start JARVIS
python start_system.py
```

### First Time Setup
1. Grant screen recording permission when prompted
2. Allow microphone access for voice commands
3. Say "Hey JARVIS" to activate
4. Try: "Hey JARVIS, describe what's on my screen"

## üéÆ Using the Vision System

### Basic Commands
- **"Describe my screen"** - General screen analysis
- **"What am I looking at?"** - Context-aware description
- **"Check for notifications"** - Scans for app notifications
- **"Analyze this window"** - Focused window analysis
- **"What's happening in Chrome?"** - App-specific analysis

### Advanced Features
- **"Monitor my workspace"** - Continuous monitoring mode
- **"Learn this pattern"** - Teach JARVIS new patterns
- **"What did you see earlier?"** - Access vision history
- **"Compare screens"** - Before/after analysis

### Autonomous Mode
Enable full autonomy for proactive vision monitoring:
```
"Hey JARVIS, activate full autonomy"
```

JARVIS will then:
- Monitor your screen continuously
- Alert you to important changes
- Proactively offer assistance
- Learn your patterns and preferences

## üîß Customization & Extension

### Adding Custom Vision Providers

1. Create a new file in `backend/vision/plugins/`
2. Extend `BaseVisionProvider`
3. Register your capabilities
4. Implement the execute method

### Training the System

The system learns automatically, but you can accelerate learning:
- Provide feedback on results
- Use consistent command patterns
- Correct misunderstandings
- The more you use it, the smarter it gets!

### Configuration

Vision system configuration in `backend/vision/config.json`:
```json
{
  "semantic_model": "all-MiniLM-L6-v2",
  "confidence_threshold": 0.7,
  "learning_rate": 0.1,
  "cache_duration": 30,
  "parallel_providers": true
}
```

## üìä Architecture Benefits

### **Advantages of Zero-Hardcoding Design**

1. **Infinite Extensibility**: Add new capabilities without changing core code
2. **Natural Language**: Understands variations and synonyms automatically
3. **Self-Improving**: Gets better with use, no manual updates needed
4. **Language Agnostic**: Works with any language (ML-based understanding)
5. **Future Proof**: Adapts to new use cases automatically

### **Performance Characteristics**

- **Intent Analysis**: 10-50ms (with semantic embeddings)
- **Capability Scoring**: 5-20ms (with caching)
- **Provider Execution**: Varies by provider (typically 50-500ms)
- **Learning Updates**: Asynchronous, non-blocking
- **Memory Usage**: ~200MB for ML models, scales with usage

## üõ†Ô∏è Troubleshooting

### Vision System Not Responding
```bash
# Run diagnostic
python backend/diagnose_vision.py

# Check specific component
python backend/test_vision_simple.py
```

### Learning System Issues
```bash
# Reset learning database
rm backend/data/vision_learning.json

# View learning statistics
curl http://localhost:8000/vision/stats
```

### Plugin Not Loading
- Check file is in `backend/vision/plugins/`
- Ensure class extends `BaseVisionProvider`
- Check logs for import errors

## üöÄ Future Roadmap

### Coming Soon
- **Visual GUI for Training** - Teach JARVIS visually
- **Multi-Modal Fusion** - Combine vision with other senses
- **Distributed Learning** - Share learned patterns (opt-in)
- **Real-time Collaboration** - Multiple users training together
- **Vision Transformers** - State-of-the-art vision models

## ü§ù Contributing

We welcome contributions! The plugin architecture makes it easy:

1. Fork the repository
2. Create a feature branch
3. Add your vision provider plugin
4. Submit a pull request

## üìÑ License

MIT License - Feel free to use in your own projects!

## üôè Acknowledgments

- Anthropic for Claude AI
- The open-source ML community
- Tony Stark for the inspiration

---

<p align="center">
  <strong>Built with ‚ù§Ô∏è by the JARVIS Team</strong><br>
  <em>"The future of AI assistants is here, and it learns from you."</em>
</p>