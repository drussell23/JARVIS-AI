name: Claude AI PR Analyzer

on:
  pull_request:
    types: [opened, synchronize, reopened]
  pull_request_review_comment:
    types: [created]
  issue_comment:
    types: [created]

permissions:
  contents: write
  pull-requests: write
  issues: write

jobs:
  analyze-pr:
    name: AI-Powered PR Analysis
    runs-on: ubuntu-latest
    if: |
      (github.event_name == 'pull_request' &&
       github.event.pull_request.changed_files < 50) ||
      (github.event_name == 'issue_comment' &&
       contains(github.event.comment.body, '@claude') &&
       github.event.issue.pull_request)

    steps:
      - name: Checkout PR
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.pull_request.head.sha }}
          fetch-depth: 0

      - name: Get PR Number
        id: pr_number
        uses: actions/github-script@v7
        with:
          script: |
            const pr = context.payload.pull_request ||
                      (await github.rest.pulls.get({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number
                      })).data;

            return pr.number;

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.10'

      - name: Install Dependencies
        run: |
          pip install anthropic pygithub gitpython

      - name: Run Claude AI Analysis
        id: ai_analysis
        env:
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          PR_NUMBER: ${{ steps.pr_number.outputs.result }}
        run: |
          python3 << 'PYTHON_SCRIPT'
          import anthropic
          import os
          import json
          import sys
          import re
          import asyncio
          from pathlib import Path
          from typing import List, Dict, Optional, Tuple
          from dataclasses import dataclass
          from github import Github, GithubException
          import concurrent.futures

          # Initialize clients
          claude_client = anthropic.Anthropic(api_key=os.environ['ANTHROPIC_API_KEY'])
          github_client = Github(os.environ['GITHUB_TOKEN'])

          # Get repository and PR
          repo = github_client.get_repo(os.environ['GITHUB_REPOSITORY'])
          pr_number = int(os.environ['PR_NUMBER'])
          pr = repo.get_pull(pr_number)

          print(f"üîç Analyzing PR #{pr_number}: {pr.title}")
          print(f"üìä Initial stats: {pr.changed_files} files, +{pr.additions}/-{pr.deletions} lines")

          # File filtering patterns - skip these
          SKIP_PATTERNS = [
              r'\.lock$',           # Lock files
              r'\.min\.(js|css)$',  # Minified files
              r'^dist/',            # Distribution builds
              r'^build/',           # Build artifacts
              r'node_modules/',     # Dependencies
              r'__pycache__/',      # Python cache
              r'\.pyc$',            # Compiled Python
              r'^venv/',            # Virtual environments
              r'\.(png|jpg|jpeg|gif|svg|ico)$',  # Images
              r'\.(pdf|zip|tar|gz)$',             # Binaries
          ]

          # Documentation file patterns - analyze differently
          DOC_PATTERNS = [
              r'\.md$',
              r'^docs/',
              r'README',
              r'CHANGELOG',
              r'LICENSE',
          ]

          def should_skip_file(filename):
              """Check if file should be skipped entirely."""
              for pattern in SKIP_PATTERNS:
                  if re.search(pattern, filename, re.IGNORECASE):
                      return True
              return False

          def is_doc_file(filename):
              """Check if file is documentation."""
              for pattern in DOC_PATTERNS:
                  if re.search(pattern, filename, re.IGNORECASE):
                      return True
              return False

          def estimate_tokens(text):
              """Rough token estimation (1 token ‚âà 4 characters)."""
              return len(text) // 4

          def truncate_content(content, max_tokens=2000):
              """Intelligently truncate content while preserving structure."""
              if estimate_tokens(content) <= max_tokens:
                  return content, False

              lines = content.split('\n')
              if len(lines) <= 100:
                  # Small file, just truncate
                  max_chars = max_tokens * 4
                  return content[:max_chars] + '\n... [truncated]', True

              # For large files, keep beginning and end
              keep_lines = 50
              truncated = '\n'.join([
                  *lines[:keep_lines],
                  f'\n... [{len(lines) - (keep_lines * 2)} lines truncated] ...\n',
                  *lines[-keep_lines:]
              ])
              return truncated, True

          def truncate_patch(patch, max_lines=30):
              """Truncate diff patch to show most relevant changes."""
              if not patch:
                  return patch, False

              lines = patch.split('\n')
              if len(lines) <= max_lines:
                  return patch, False

              # Keep first and last portions of patch
              keep = max_lines // 2
              truncated = '\n'.join([
                  *lines[:keep],
                  f'... [{len(lines) - (keep * 2)} lines truncated] ...',
                  *lines[-keep:]
              ])
              return truncated, True

          @dataclass
          class FileAnalysis:
              """Structured file analysis data."""
              path: str
              status: str
              additions: int
              deletions: int
              patch: str
              content_preview: str
              was_truncated: bool
              tokens: int
              file_type: str  # 'code', 'doc', 'skipped'

          def process_file(file_obj) -> Optional[FileAnalysis]:
              """Process a single file with intelligent filtering."""
              file_path = file_obj.filename

              # Skip filtered files
              if should_skip_file(file_path):
                  return FileAnalysis(
                      path=file_path, status='skipped', additions=0, deletions=0,
                      patch='', content_preview='', was_truncated=False,
                      tokens=0, file_type='skipped'
                  )

              # Handle documentation files
              if is_doc_file(file_path):
                  return FileAnalysis(
                      path=file_path, status=file_obj.status,
                      additions=file_obj.additions, deletions=file_obj.deletions,
                      patch='', content_preview='', was_truncated=False,
                      tokens=0, file_type='doc'
                  )

              # Process code files
              try:
                  if Path(file_path).exists():
                      content = Path(file_path).read_text(encoding='utf-8', errors='ignore')
                  else:
                      content = "[File deleted]"
              except Exception as e:
                  content = f"[Error: {str(e)[:100]}]"

              # Truncate intelligently
              content_truncated, content_was_truncated = truncate_content(content, max_tokens=2000)

              # Get patch with size limit
              try:
                  patch = file_obj.patch if hasattr(file_obj, 'patch') and file_obj.patch else ''
              except:
                  patch = ''

              patch_truncated, patch_was_truncated = truncate_patch(patch, max_lines=30)

              file_tokens = estimate_tokens(content_truncated) + estimate_tokens(patch_truncated)

              return FileAnalysis(
                  path=file_path,
                  status=file_obj.status,
                  additions=file_obj.additions,
                  deletions=file_obj.deletions,
                  patch=patch_truncated,
                  content_preview=content_truncated,
                  was_truncated=content_was_truncated or patch_was_truncated,
                  tokens=file_tokens,
                  file_type='code'
              )

          # Fetch and process files with parallel execution
          print(f"üîÑ Fetching PR files...")
          pr_files = list(pr.get_files())
          print(f"üìä Processing {len(pr_files)} files in parallel...")

          # Process files in parallel for speed
          processed_files = []
          with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
              future_to_file = {executor.submit(process_file, f): f for f in pr_files}
              for future in concurrent.futures.as_completed(future_to_file):
                  try:
                      result = future.result()
                      if result:
                          processed_files.append(result)
                  except Exception as e:
                      file_obj = future_to_file[future]
                      print(f"‚ö†Ô∏è  Error processing {file_obj.filename}: {e}")

          # Categorize files
          code_files = []
          doc_files = []
          skipped_files = []
          total_tokens = 0
          MAX_TOTAL_TOKENS = 40000

          for file in processed_files:
              if file.file_type == 'skipped':
                  skipped_files.append(file.path)
                  print(f"‚è≠Ô∏è  Skipped: {file.path}")
              elif file.file_type == 'doc':
                  doc_files.append(file)
                  print(f"üìù Doc: {file.path}")
              elif file.file_type == 'code':
                  # Check token budget
                  if total_tokens + file.tokens > MAX_TOTAL_TOKENS:
                      print(f"‚ö†Ô∏è  Token budget exceeded at {file.path}")
                      skipped_files.append(f"{file.path} (token limit)")
                      continue

                  code_files.append(file)
                  total_tokens += file.tokens
                  print(f"‚úÖ Analyzed: {file.path} (~{file.tokens} tokens)")

          # Calculate totals
          total_additions = sum(f.additions for f in processed_files)
          total_deletions = sum(f.deletions for f in processed_files)

          print(f"\nüìä Processing Summary:")
          print(f"  - Total files in PR: {len(pr_files)}")
          print(f"  - Code files analyzed: {len(code_files)}")
          print(f"  - Documentation files: {len(doc_files)}")
          print(f"  - Skipped files: {len(skipped_files)}")
          print(f"  - Total tokens: ~{total_tokens:,}")
          print(f"  - Lines: +{total_additions:,}/-{total_deletions:,}")

          # Prepare context for Claude
          pr_body = pr.body or "No description provided"
          context = f"""# Pull Request Analysis Request

          ## PR Information
          - **Title:** {pr.title}
          - **Author:** {pr.user.login}
          - **Base Branch:** {pr.base.ref}
          - **Head Branch:** {pr.head.ref}
          - **Description:** {pr_body[:500]}{'...' if len(pr_body) > 500 else ''}

          ## Changes Summary
          - **Total Files Changed:** {len(processed_files)}
          - **Code Files Analyzed:** {len(code_files)}
          - **Documentation Files:** {len(doc_files)} (summarized below)
          - **Skipped Files:** {len(skipped_files)}
          - **Lines Added:** {total_additions}
          - **Lines Deleted:** {total_deletions}

          """

          # Add documentation summary if present
          if doc_files:
              context += "\n## Documentation Changes\n"
              context += f"This PR includes {len(doc_files)} documentation file(s):\n"
              for doc in doc_files[:5]:  # Limit to 5
                  context += f"- {doc.path}: +{doc.additions} -{doc.deletions} lines\n"
              if len(doc_files) > 5:
                  context += f"- ... and {len(doc_files) - 5} more documentation files\n"
              context += "\n**Note:** Focus analysis on code changes; documentation appears comprehensive.\n"

          # Add code file details
          if code_files:
              context += "\n## Code Files Changed\n"
              for file in code_files[:15]:  # Limit to 15 files for analysis
                  context += f"""
          ### {file.path}
          - **Status:** {file.status}
          - **Changes:** +{file.additions} -{file.deletions}
          {'- **Note:** Content truncated for analysis' if file.was_truncated else ''}

          **Diff:**
          ```diff
          {file.patch if file.patch else 'No diff available'}
          ```

          **Current Content:**
          ```
          {file.content_preview}
          ```
          """

              if len(code_files) > 15:
                  context += f"\n... and {len(code_files) - 15} more code files not shown due to size limits.\n"

          # Add skipped files note
          if skipped_files:
              context += f"\n## Files Not Analyzed\n"
              context += f"Skipped {len(skipped_files)} file(s): {', '.join(skipped_files[:10])}"
              if len(skipped_files) > 10:
                  context += f" and {len(skipped_files) - 10} more"
              context += "\n"

          # Call Claude for analysis
          print("ü§ñ Calling Claude AI for analysis...")

          # Adjust analysis based on PR type
          is_doc_heavy = len(doc_files) > len(code_files) and len(doc_files) > 3

          if is_doc_heavy:
              system_prompt = """You are an expert documentation reviewer analyzing a documentation-heavy pull request.

          This PR is primarily documentation changes. Focus your analysis on:
          - **Documentation Quality:** Clarity, completeness, accuracy
          - **Structure:** Organization, navigation, consistency
          - **Code Examples:** Correctness of any code snippets in docs
          - **Accessibility:** Readability for target audience

          Provide a concise review with:
          1. **Overall Assessment** (score 1-10)
          2. **Documentation Quality** (clarity, completeness, accuracy)
          3. **Suggestions** (improvements, corrections, additions)
          4. **Merge Decision** (ready/needs-work)

          Keep response concise since this is primarily documentation.
          Format in clear Markdown with sections and emojis."""
          else:
              system_prompt = """You are an expert code reviewer and software architect analyzing a pull request for the JARVIS AI Agent project.

          Your analysis should be:
          - **Comprehensive:** Cover code quality, architecture, security, performance, and best practices
          - **Actionable:** Provide specific suggestions with code examples
          - **Constructive:** Focus on improvements while acknowledging good work
          - **Context-aware:** Understand this is an AI agent system with hybrid architecture
          - **Security-focused:** Identify any security vulnerabilities or credential exposure
          - **Performance-conscious:** Highlight performance issues or optimizations

          Analyze the PR and provide:
          1. **Overall Assessment** (score 1-10 and summary)
          2. **Code Quality Analysis** (style, readability, maintainability)
          3. **Architecture Impact** (how it fits with existing system)
          4. **Security Review** (vulnerabilities, best practices)
          5. **Performance Analysis** (bottlenecks, optimizations)
          6. **Test Coverage** (adequacy of tests)
          7. **Specific Issues Found** (with severity: critical/major/minor)
          8. **Recommendations** (prioritized action items)
          9. **Merge Decision** (ready/needs-work/blocking-issues)

          Format your response in clear Markdown with sections, code blocks, and emojis."""

          message = claude_client.messages.create(
              model="claude-sonnet-4-20250514",
              max_tokens=8000,
              temperature=0,
              system=system_prompt,
              messages=[
                  {
                      "role": "user",
                      "content": context
                  }
              ]
          )

          analysis = message.content[0].text

          # Save analysis to file
          with open('/tmp/claude_analysis.md', 'w') as f:
              f.write(analysis)

          # Extract merge decision
          merge_ready = "ready" in analysis.lower() and "merge decision" in analysis.lower()

          # Calculate token usage
          input_tokens = message.usage.input_tokens
          output_tokens = message.usage.output_tokens
          total_tokens = input_tokens + output_tokens
          estimated_cost = (input_tokens * 0.003 / 1000) + (output_tokens * 0.015 / 1000)

          # Output for GitHub Actions
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"merge_ready={str(merge_ready).lower()}\n")
              f.write(f"analysis_length={len(analysis)}\n")
              f.write(f"code_files={len(code_files)}\n")
              f.write(f"doc_files={len(doc_files)}\n")
              f.write(f"skipped_files={len(skipped_files)}\n")
              f.write(f"input_tokens={input_tokens}\n")
              f.write(f"output_tokens={output_tokens}\n")
              f.write(f"total_tokens={total_tokens}\n")
              f.write(f"estimated_cost={estimated_cost:.4f}\n")

          print("‚úÖ Claude AI analysis complete!")
          print(f"üìä Statistics:")
          print(f"  - Analysis length: {len(analysis)} characters")
          print(f"  - Code files analyzed: {len(code_files)}")
          print(f"  - Documentation files: {len(doc_files)}")
          print(f"  - Skipped files: {len(skipped_files)}")
          print(f"  - Input tokens: {input_tokens:,}")
          print(f"  - Output tokens: {output_tokens:,}")
          print(f"  - Total tokens: {total_tokens:,}")
          print(f"  - Estimated cost: ${estimated_cost:.4f}")
          print(f"  - Merge ready: {merge_ready}")

          PYTHON_SCRIPT

      - name: Post AI Analysis as Comment
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('/tmp/claude_analysis.md', 'utf8');

            const pr = context.payload.pull_request ||
                      (await github.rest.pulls.get({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number
                      })).data;

            // Find existing Claude comment
            const comments = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: pr.number
            });

            const claudeComment = comments.data.find(c =>
              c.user.type === 'Bot' &&
              c.body.includes('ü§ñ Claude AI Code Review')
            );

            const commentBody = `## ü§ñ Claude AI Code Review

            ${analysis}

            ---
            <sub>Powered by Claude Sonnet 4 | Generated at ${new Date().toUTCString()}</sub>
            <sub>Mention @claude in comments to re-trigger analysis</sub>`;

            if (claudeComment) {
              // Update existing comment
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: claudeComment.id,
                body: commentBody
              });
            } else {
              // Create new comment
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                body: commentBody
              });
            }

      - name: Add Labels Based on AI Analysis
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const analysis = fs.readFileSync('/tmp/claude_analysis.md', 'utf8').toLowerCase();

            const pr = context.payload.pull_request ||
                      (await github.rest.pulls.get({
                        owner: context.repo.owner,
                        repo: context.repo.repo,
                        pull_number: context.issue.number
                      })).data;

            const labels = [];

            // Intelligent label detection
            if (analysis.includes('security') || analysis.includes('vulnerability')) {
              labels.push('security');
            }
            if (analysis.includes('performance') || analysis.includes('optimization')) {
              labels.push('performance');
            }
            if (analysis.includes('breaking') || analysis.includes('breaking change')) {
              labels.push('breaking-change');
            }
            if (analysis.includes('needs work') || analysis.includes('blocking')) {
              labels.push('needs-work');
            }
            if (analysis.includes('ready') && analysis.includes('merge decision')) {
              labels.push('ready-to-merge');
            }
            if (analysis.includes('test') && (analysis.includes('missing') || analysis.includes('inadequate'))) {
              labels.push('needs-tests');
            }
            if (analysis.includes('documentation') || analysis.includes('docs')) {
              labels.push('documentation');
            }

            if (labels.length > 0) {
              await github.rest.issues.addLabels({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: pr.number,
                labels: labels
              });
            }

      - name: Create Review with AI Feedback
        if: steps.ai_analysis.outputs.merge_ready == 'false'
        uses: actions/github-script@v7
        with:
          script: |
            const pr = context.payload.pull_request;
            if (!pr) return;

            await github.rest.pulls.createReview({
              owner: context.repo.owner,
              repo: context.repo.repo,
              pull_number: pr.number,
              event: 'REQUEST_CHANGES',
              body: 'ü§ñ Claude AI has identified issues that need to be addressed before merging. Please review the detailed analysis above.'
            });

      - name: Summary
        run: |
          echo "## ü§ñ Claude AI Analysis Complete" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üìä Analysis Metrics" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Code Files Analyzed | ${{ steps.ai_analysis.outputs.code_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Documentation Files | ${{ steps.ai_analysis.outputs.doc_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Files Skipped | ${{ steps.ai_analysis.outputs.skipped_files }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Analysis Length | ${{ steps.ai_analysis.outputs.analysis_length }} chars |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### üí∞ Token Usage & Cost" >> $GITHUB_STEP_SUMMARY
          echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
          echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
          echo "| Input Tokens | ${{ steps.ai_analysis.outputs.input_tokens }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Output Tokens | ${{ steps.ai_analysis.outputs.output_tokens }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Total Tokens | ${{ steps.ai_analysis.outputs.total_tokens }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Estimated Cost | \$${{ steps.ai_analysis.outputs.estimated_cost }} |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ‚úÖ Merge Decision" >> $GITHUB_STEP_SUMMARY
          echo "**Merge Ready:** ${{ steps.ai_analysis.outputs.merge_ready }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Full analysis posted as PR comment." >> $GITHUB_STEP_SUMMARY
