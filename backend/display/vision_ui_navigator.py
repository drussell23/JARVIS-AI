#!/usr/bin/env python3
"""
Vision-Guided UI Navigator for Display Connection
==================================================

Uses JARVIS Vision to navigate macOS Control Center and connect to displays.
Bypasses all macOS Sequoia security restrictions by using visual recognition.

How it works:
1. Capture screen with existing vision system
2. Use Claude Vision to identify UI elements
3. Calculate click coordinates from bounding boxes
4. Execute mouse clicks with PyAutoGUI
5. Verify actions completed successfully
6. Monitor connection progress visually

Features:
- Zero hardcoding - fully configuration-driven
- Async/await support throughout
- Self-healing with retry logic
- Comprehensive visual verification
- Integration with existing JARVIS vision system
- Works on macOS Sequoia without accessibility permissions

Author: Derek Russell
Date: 2025-10-15
Version: 2.0
"""

import asyncio
import logging
import subprocess
import json
import pyautogui
import time
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from PIL import Image
import base64
import io
import re

logger = logging.getLogger(__name__)


@dataclass
class UIElement:
    """Visual UI element detected by Claude Vision"""
    name: str
    description: str
    bounding_box: Optional[Tuple[int, int, int, int]]  # (x, y, width, height)
    center_point: Optional[Tuple[int, int]]  # (x, y)
    confidence: float
    element_type: str  # icon, button, text, menu_item


@dataclass
class NavigationResult:
    """Result of a navigation attempt"""
    success: bool
    message: str
    steps_completed: List[str]
    duration: float
    screenshot_path: Optional[str] = None
    error_details: Optional[Dict[str, Any]] = None


class VisionUINavigator:
    """
    Vision-guided UI navigator
    
    Uses Claude Vision to see and interact with macOS UI elements.
    Perfect for bypassing macOS Sequoia security restrictions.
    """
    
    def __init__(self, config_path: Optional[str] = None):
        """Initialize vision navigator"""
        # Load configuration
        if config_path is None:
            config_path = Path(__file__).parent.parent / 'config' / 'vision_navigator_config.json'
        
        self.config = self._load_config(config_path)
        self.screenshots_dir = Path.home() / '.jarvis' / 'screenshots' / 'ui_navigation'
        self.screenshots_dir.mkdir(parents=True, exist_ok=True)
        
        # Vision analyzer (will be set by display monitor)
        self.vision_analyzer = None
        
        # Enhanced Vision Pipeline (v1.0)
        self.enhanced_pipeline = None
        self.use_enhanced_pipeline = self.config.get('advanced', {}).get('use_enhanced_pipeline', True)
        
        # Statistics
        self.stats = {
            'total_navigations': 0,
            'successful': 0,
            'failed': 0,
            'avg_duration': 0.0,
            'enhanced_pipeline_used': 0,
            'fallback_used': 0
        }
        
        # Configure PyAutoGUI safety
        pyautogui.PAUSE = self.config.get('mouse', {}).get('delay_between_actions', 0.5)
        pyautogui.FAILSAFE = True
        
        logger.info("[VISION NAV] Vision UI Navigator initialized")
        logger.info(f"[VISION NAV] Enhanced Pipeline: {'enabled' if self.use_enhanced_pipeline else 'disabled'}")
    
    def _load_config(self, config_path: Path) -> Dict[str, Any]:
        """Load configuration"""
        try:
            with open(config_path) as f:
                config = json.load(f)
            logger.info(f"[VISION NAV] Loaded config from {config_path}")
            return config
        except FileNotFoundError:
            logger.warning(f"[VISION NAV] Config not found, using defaults")
            return self._get_default_config()
        except Exception as e:
            logger.error(f"[VISION NAV] Error loading config: {e}")
            return self._get_default_config()
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration"""
        return {
            "navigation": {
                "max_retries": 3,
                "retry_delay": 1.0,
                "screenshot_verification": True,
                "step_delay": 0.8,
                "max_navigation_time": 30.0
            },
            "mouse": {
                "delay_between_actions": 0.5,
                "click_duration": 0.1,
                "movement_speed": 0.3
            },
            "vision": {
                "confidence_threshold": 0.7,
                "use_bounding_boxes": True,
                "verify_actions": True,
                "screenshot_format": "png",
                "screenshot_quality": 95
            },
            "prompts": {
                "find_control_center": "Find the Control Center icon in the menu bar (top right, looks like two overlapping rectangles). Provide the exact pixel coordinates of its center.",
                "find_screen_mirroring": "Find the Screen Mirroring button in Control Center (looks like two overlapping screens). Provide the exact pixel coordinates of its center.",
                "find_display": "Find '{display_name}' in the list of available displays. Provide the exact pixel coordinates to click on it."
            }
        }
    
    def set_vision_analyzer(self, analyzer):
        """Set Claude Vision analyzer instance"""
        self.vision_analyzer = analyzer
        logger.info("[VISION NAV] Vision analyzer connected")
        
        # Initialize Enhanced Vision Pipeline
        if self.use_enhanced_pipeline:
            asyncio.create_task(self._initialize_enhanced_pipeline())
    
    async def _initialize_enhanced_pipeline(self):
        """Initialize Enhanced Vision Pipeline"""
        try:
            from vision.enhanced_vision_pipeline import get_vision_pipeline
            
            self.enhanced_pipeline = get_vision_pipeline()
            
            # Initialize all stages
            initialized = await self.enhanced_pipeline.initialize()
            
            if initialized:
                # Connect Claude Vision to validator
                if self.enhanced_pipeline.model_validator and self.vision_analyzer:
                    self.enhanced_pipeline.model_validator.set_claude_analyzer(self.vision_analyzer)
                
                logger.info("[VISION NAV] ‚úÖ Enhanced Vision Pipeline v1.0 initialized")
                logger.info("[VISION NAV] üöÄ 5-stage pipeline ready:")
                logger.info("[VISION NAV]    Stage 1: Screen Region Segmentation (Quadtree)")
                logger.info("[VISION NAV]    Stage 2: Icon Pattern Recognition (OpenCV + Edge)")
                logger.info("[VISION NAV]    Stage 3: Coordinate Calculation (Physics-based)")
                logger.info("[VISION NAV]    Stage 4: Multi-Model Validation (Monte Carlo)")
                logger.info("[VISION NAV]    Stage 5: Mouse Automation (Bezier trajectories)")
            else:
                logger.warning("[VISION NAV] Enhanced Pipeline initialization failed, using fallback")
                self.use_enhanced_pipeline = False
                
        except Exception as e:
            logger.warning(f"[VISION NAV] Could not initialize Enhanced Pipeline: {e}")
            logger.info("[VISION NAV] Using fallback navigation methods")
            self.use_enhanced_pipeline = False
    
    async def connect_to_display(self, display_name: str) -> NavigationResult:
        """
        Connect to a display using vision-guided navigation
        
        Args:
            display_name: Name of display to connect to (e.g., "Living Room TV")
            
        Returns:
            NavigationResult with success status and details
        """
        start_time = time.time()
        steps_completed = []
        self.stats['total_navigations'] += 1
        
        logger.info(f"[VISION NAV] Starting vision-guided connection to '{display_name}'")
        
        try:
            # Step 1: Find and click Control Center icon
            logger.info("[VISION NAV] Step 1: Finding Control Center icon...")
            cc_clicked = await self._find_and_click_control_center()
            if not cc_clicked:
                raise Exception("Could not find or click Control Center icon")
            steps_completed.append("control_center_opened")
            
            # Wait for Control Center to open
            await asyncio.sleep(self.config['navigation']['step_delay'])
            
            # Step 2: Find and click Screen Mirroring button
            logger.info("[VISION NAV] Step 2: Finding Screen Mirroring button...")
            sm_clicked = await self._find_and_click_screen_mirroring()
            if not sm_clicked:
                raise Exception("Could not find or click Screen Mirroring button")
            steps_completed.append("screen_mirroring_opened")
            
            # Wait for Screen Mirroring menu to open
            await asyncio.sleep(self.config['navigation']['step_delay'])
            
            # Step 3: Find and click display
            logger.info(f"[VISION NAV] Step 3: Finding '{display_name}' in list...")
            display_clicked = await self._find_and_click_display(display_name)
            if not display_clicked:
                raise Exception(f"Could not find or click '{display_name}' in display list")
            steps_completed.append("display_selected")
            
            # Step 4: Verify connection
            logger.info("[VISION NAV] Step 4: Verifying connection...")
            await asyncio.sleep(2.0)  # Wait for connection to establish
            
            connected = await self._verify_connection(display_name)
            if connected:
                steps_completed.append("connection_verified")
            
            duration = time.time() - start_time
            self.stats['successful'] += 1
            self.stats['avg_duration'] = (
                (self.stats['avg_duration'] * (self.stats['successful'] - 1) + duration) 
                / self.stats['successful']
            )
            
            logger.info(f"[VISION NAV] ‚úÖ Successfully connected to '{display_name}' in {duration:.2f}s")
            
            return NavigationResult(
                success=True,
                message=f"Successfully connected to {display_name} using vision navigation",
                steps_completed=steps_completed,
                duration=duration
            )
            
        except Exception as e:
            self.stats['failed'] += 1
            duration = time.time() - start_time
            
            logger.error(f"[VISION NAV] ‚ùå Navigation failed: {e}")
            
            return NavigationResult(
                success=False,
                message=f"Vision navigation failed: {str(e)}",
                steps_completed=steps_completed,
                duration=duration,
                error_details={'exception': str(e), 'steps_completed': steps_completed}
            )
    
    async def _find_and_click_control_center(self) -> bool:
        """Find and click Control Center icon in menu bar using ML-powered detection"""
        try:
            # Try Enhanced Vision Pipeline with ML Template Generator first
            if self.use_enhanced_pipeline and self.enhanced_pipeline:
                logger.info("[VISION NAV] üöÄ Using Enhanced Vision Pipeline v1.0 with ML Template Generator")

                result = await self.enhanced_pipeline.execute_pipeline(
                    target='control_center',
                    context={'navigator': self}
                )

                if result.success:
                    self.stats['enhanced_pipeline_used'] += 1
                    logger.info(f"[VISION NAV] ‚úÖ Enhanced Pipeline + ML succeeded in {result.execution_time_ms:.1f}ms")
                    return True
                else:
                    logger.warning(f"[VISION NAV] Enhanced Pipeline failed: {result.error}")
                    logger.info("[VISION NAV] Falling back to direct ML detection")

            # Fallback 1: Direct ML Template Detection
            try:
                from vision.enhanced_vision_pipeline.icon_detection_engine import IconDetectionEngine
                from vision.enhanced_vision_pipeline.screen_region_analyzer import ScreenRegion

                # Capture screen
                screenshot = await self._capture_screen()
                if not screenshot:
                    raise Exception("Failed to capture screen")

                # Crop to menu bar region only (top 50px of screen)
                # This makes detection much faster and more accurate
                menu_bar_height = 50
                menu_bar_screenshot = screenshot.crop((0, 0, screenshot.width, menu_bar_height))

                logger.info(f"[VISION NAV] Cropped to menu bar region: {menu_bar_screenshot.size}")

                # Create screen region for detection
                screen_region = ScreenRegion(
                    image=menu_bar_screenshot,
                    bounds=(0, 0, menu_bar_screenshot.width, menu_bar_screenshot.height),
                    region_type='menu_bar',
                    confidence=1.0
                )

                # Initialize icon detector with ML template generator
                detector = IconDetectionEngine(self.config)
                await detector.initialize()

                logger.info("[VISION NAV] üéØ Using ML Template Generator for Control Center detection")

                # Detect Control Center icon
                detection_result = await detector.detect_icon(
                    region=screen_region,
                    target='control_center',
                    context={'navigator': self}
                )

                if detection_result.get('success'):
                    best_result = detection_result['detection_results']['best']

                    if best_result.found and best_result.center_point:
                        x, y = best_result.center_point
                        logger.info(f"[VISION NAV] ‚úÖ ML Template detected Control Center at ({x}, {y})")
                        logger.info(f"[VISION NAV]    Confidence: {best_result.confidence:.2%}, Method: {best_result.method}")

                        # Click the icon
                        await self._click_at(x, y)

                        self.stats['enhanced_pipeline_used'] += 1
                        return True

            except Exception as ml_error:
                logger.warning(f"[VISION NAV] ML detection failed: {ml_error}")

            # Fallback 2: Claude Vision (ENHANCED with better prompt)
            self.stats['fallback_used'] += 1

            # Capture full screen (not cropped - Claude needs full context)
            screenshot = await self._capture_screen()

            if not screenshot:
                logger.error("[VISION NAV] Failed to capture screen")
                return False

            # Crop to menu bar for Claude Vision (faster analysis)
            menu_bar_screenshot = screenshot.crop((0, 0, screenshot.width, 50))

            # Save screenshot for analysis
            screenshot_path = self.screenshots_dir / f'control_center_search_{int(time.time())}.png'
            menu_bar_screenshot.save(screenshot_path)

            if self.vision_analyzer:
                logger.info("[VISION NAV] Using Claude Vision to locate Control Center icon...")

                # Enhanced prompt with specific instructions
                prompt = """Find the Control Center icon in this macOS menu bar.

The Control Center icon looks like two overlapping rounded rectangles (like a toggle switch).
It's typically located in the RIGHT section of the menu bar, near the WiFi, Battery, and Time icons.

Please provide the EXACT pixel coordinates where I should click to open Control Center.
Respond in this format: X_POSITION: [x coordinate], Y_POSITION: [y coordinate]

For example: X_POSITION: 545, Y_POSITION: 15"""

                # Analyze with Claude Vision
                analysis = await self._analyze_with_vision(screenshot_path, prompt)

                logger.info(f"[VISION NAV] Claude Vision response: {analysis[:200] if analysis else 'None'}...")

                # Extract coordinates from analysis
                coords = self._extract_coordinates_from_response(analysis)

                if coords:
                    x, y = coords
                    logger.info(f"[VISION NAV] ‚úÖ Claude Vision found Control Center at ({x}, {y})")

                    # Verify coordinates are reasonable for menu bar
                    if y > 40:
                        logger.warning(f"[VISION NAV] ‚ö†Ô∏è Suspicious Y coordinate: {y} (should be <40 for menu bar)")
                        logger.info("[VISION NAV] Adjusting Y to 15 (menu bar center)")
                        y = 15

                    # Click the icon
                    await self._click_at(x, y)

                    return True
                else:
                    logger.warning("[VISION NAV] Could not extract coordinates from Claude Vision response")

            # Final fallback: Use heuristic (top-right area of menu bar)
            logger.info("[VISION NAV] Using heuristic fallback for Control Center")
            return await self._click_control_center_heuristic()

        except Exception as e:
            logger.error(f"[VISION NAV] Error finding Control Center: {e}")
            return False
    
    async def _find_and_click_screen_mirroring(self) -> bool:
        """Find and click Screen Mirroring button in Control Center using ML-powered detection"""
        try:
            # Let UI settle after Control Center opens
            await asyncio.sleep(0.3)

            # Capture screen
            screenshot = await self._capture_screen()
            if not screenshot:
                return False

            # Try ML Template Detection first
            try:
                from vision.enhanced_vision_pipeline.icon_detection_engine import IconDetectionEngine
                from vision.enhanced_vision_pipeline.screen_region_analyzer import ScreenRegion

                # Create screen region for detection
                screen_region = ScreenRegion(
                    image=screenshot,
                    bounds=(0, 0, screenshot.width, screenshot.height),
                    region_type='control_center',
                    confidence=1.0
                )

                # Initialize icon detector with ML template generator
                detector = IconDetectionEngine(self.config)
                await detector.initialize()

                logger.info("[VISION NAV] üéØ Using ML Template Generator for Screen Mirroring detection")

                # Detect Screen Mirroring button
                detection_result = await detector.detect_icon(
                    region=screen_region,
                    target='screen_mirroring',
                    context={'navigator': self}
                )

                if detection_result.get('success'):
                    best_result = detection_result['detection_results']['best']

                    if best_result.found and best_result.center_point:
                        x, y = best_result.center_point
                        logger.info(f"[VISION NAV] ‚úÖ ML Template detected Screen Mirroring at ({x}, {y})")
                        logger.info(f"[VISION NAV]    Confidence: {best_result.confidence:.2%}, Method: {best_result.method}")

                        # Click the button
                        await self._click_at(x, y)

                        return True

            except Exception as ml_error:
                logger.warning(f"[VISION NAV] ML detection failed: {ml_error}")

            # Fallback 1: Claude Vision
            screenshot_path = self.screenshots_dir / f'screen_mirroring_search_{int(time.time())}.png'
            screenshot.save(screenshot_path)

            prompt = self.config['prompts']['find_screen_mirroring']

            if self.vision_analyzer:
                logger.info("[VISION NAV] Using Claude Vision to locate Screen Mirroring button...")

                analysis = await self._analyze_with_vision(screenshot_path, prompt)
                coords = self._extract_coordinates_from_response(analysis)

                if coords:
                    x, y = coords
                    logger.info(f"[VISION NAV] Screen Mirroring found at ({x}, {y})")

                    # Click the button
                    await self._click_at(x, y)

                    return True

            # Fallback 2: OCR search for text "Screen Mirroring" or "Display"
            logger.info("[VISION NAV] Using OCR fallback for Screen Mirroring")
            return await self._click_screen_mirroring_ocr(screenshot)

        except Exception as e:
            logger.error(f"[VISION NAV] Error finding Screen Mirroring: {e}")
            return False
    
    async def _find_and_click_display(self, display_name: str) -> bool:
        """Find and click display in the list"""
        try:
            # Capture screen with display list
            await asyncio.sleep(0.3)
            screenshot = await self._capture_screen()
            
            if not screenshot:
                return False
            
            screenshot_path = self.screenshots_dir / f'display_list_{int(time.time())}.png'
            screenshot.save(screenshot_path)
            
            # Use Claude Vision to find the display
            prompt = self.config['prompts']['find_display'].format(display_name=display_name)
            
            if self.vision_analyzer:
                logger.info(f"[VISION NAV] Using Claude Vision to locate '{display_name}'...")
                
                analysis = await self._analyze_with_vision(screenshot_path, prompt)
                coords = self._extract_coordinates_from_response(analysis)
                
                if coords:
                    x, y = coords
                    logger.info(f"[VISION NAV] '{display_name}' found at ({x}, {y})")
                    
                    # Click the display
                    await self._click_at(x, y)
                    
                    return True
            
            # Fallback: OCR search for display name
            logger.info(f"[VISION NAV] Using OCR fallback to find '{display_name}'")
            return await self._click_display_ocr(screenshot, display_name)
            
        except Exception as e:
            logger.error(f"[VISION NAV] Error finding display: {e}")
            return False
    
    async def _verify_connection(self, display_name: str) -> bool:
        """Verify display connection was successful"""
        try:
            # Wait for connection to establish
            await asyncio.sleep(1.0)
            
            # Capture screen to check for connection indicators
            screenshot = await self._capture_screen()
            
            if not screenshot:
                return False
            
            screenshot_path = self.screenshots_dir / f'verification_{int(time.time())}.png'
            screenshot.save(screenshot_path)
            
            # Use Claude Vision to verify connection
            if self.vision_analyzer:
                prompt = f"Is the display '{display_name}' currently connected? Look for connection indicators, checkmarks, or the display appearing in the list of connected displays."
                
                analysis = await self._analyze_with_vision(screenshot_path, prompt)
                
                # Check if response indicates connection
                if analysis and ('yes' in analysis.lower() or 'connected' in analysis.lower() or 'checkmark' in analysis.lower()):
                    logger.info(f"[VISION NAV] ‚úÖ Connection to '{display_name}' verified")
                    return True
            
            # Fallback: Assume success if we got this far
            logger.info("[VISION NAV] Connection verification skipped (vision analyzer not available)")
            return True
            
        except Exception as e:
            logger.error(f"[VISION NAV] Error verifying connection: {e}")
            return False
    
    async def _capture_screen(self) -> Optional[Image.Image]:
        """Capture current screen using existing vision infrastructure"""
        try:
            # Try using existing reliable screenshot capture
            from vision.reliable_screenshot_capture import ReliableScreenshotCapture
            
            capture = ReliableScreenshotCapture()
            
            # Try different capture methods
            if hasattr(capture, 'capture_current_space'):
                result = await capture.capture_current_space()
            elif hasattr(capture, 'capture_screen'):
                result = await capture.capture_screen()
            elif hasattr(capture, 'capture'):
                result = capture.capture()
            else:
                # Manually call the capture method
                result = await capture.capture_with_fallback()
            
            if hasattr(result, 'success') and result.success and hasattr(result, 'image'):
                return result.image
            elif isinstance(result, Image.Image):
                return result
            
        except ImportError:
            logger.debug("[VISION NAV] ReliableScreenshotCapture not available")
        except AttributeError as e:
            logger.debug(f"[VISION NAV] Screenshot method not available: {e}")
        except Exception as e:
            logger.debug(f"[VISION NAV] Screenshot capture error: {e}")
        
        # Fallback: Use screencapture command
        try:
            temp_path = self.screenshots_dir / f'temp_{int(time.time())}.png'
            
            process = await asyncio.create_subprocess_exec(
                'screencapture', '-x', str(temp_path),
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE
            )
            
            await process.communicate()
            
            if temp_path.exists():
                image = Image.open(temp_path)
                temp_path.unlink()  # Clean up
                logger.debug(f"[VISION NAV] Screenshot captured with screencapture command")
                return image
                
        except Exception as e:
            logger.error(f"[VISION NAV] Screenshot fallback failed: {e}")
        
        return None
    
    async def _analyze_with_vision(self, image_path: Path, prompt: str) -> Optional[str]:
        """Analyze image with Claude Vision"""
        if not self.vision_analyzer:
            logger.warning("[VISION NAV] No vision analyzer available")
            return None
        
        try:
            # Load image as PIL Image (Claude Vision Analyzer expects this)
            image = Image.open(image_path)
            
            # Use analyze_screenshot method (standard for ClaudeVisionAnalyzer)
            response = await self.vision_analyzer.analyze_screenshot(
                image=image,  # Pass PIL Image directly
                prompt=prompt,
                use_cache=False  # Don't cache UI navigation prompts
            )
            
            # Handle response - analyze_screenshot returns (Dict, AnalysisMetrics)
            if isinstance(response, tuple):
                analysis_dict, metrics = response
                # Extract text from response
                if isinstance(analysis_dict, dict):
                    response_text = analysis_dict.get('response', analysis_dict.get('text', str(analysis_dict)))
                else:
                    response_text = str(analysis_dict)
            else:
                response_text = str(response)
            
            logger.info(f"[VISION NAV] Claude response: {response_text[:300] if response_text else 'None'}...")
            
            return response_text
            
        except Exception as e:
            logger.error(f"[VISION NAV] Vision analysis error: {e}", exc_info=True)
            return None
    
    def _extract_coordinates_from_response(self, response: str) -> Optional[Tuple[int, int]]:
        """Extract (x, y) coordinates from Claude Vision response"""
        if not response:
            return None
        
        try:
            # Look for patterns like:
            # - "coordinates: (1234, 56)"
            # - "x: 1234, y: 56"
            # - "at (1234, 56)"
            # - "1234, 56"
            # - "X_POSITION: 1234" and "Y_POSITION: 56"
            # - "approximately 1234 pixels from the left"
            
            # Pattern 1: (x, y) format - most explicit
            match = re.search(r'\((\d+),\s*(\d+)\)', response)
            if match:
                x, y = int(match.group(1)), int(match.group(2))
                logger.info(f"[VISION NAV] ‚úÖ Extracted coordinates (format 1): ({x}, {y})")
                return (x, y)
            
            # Pattern 2: X_POSITION: 1234, Y_POSITION: 56 format
            x_match = re.search(r'X[_\s]*POSITION:\s*(\d+)', response, re.IGNORECASE)
            y_match = re.search(r'Y[_\s]*POSITION:\s*(\d+)', response, re.IGNORECASE)
            if x_match and y_match:
                x, y = int(x_match.group(1)), int(y_match.group(1))
                logger.info(f"[VISION NAV] ‚úÖ Extracted coordinates (format 2): ({x}, {y})")
                return (x, y)
            
            # Pattern 3: x: 1234, y: 56
            match = re.search(r'x:\s*(\d+).*?y:\s*(\d+)', response, re.IGNORECASE | re.DOTALL)
            if match:
                x, y = int(match.group(1)), int(match.group(2))
                logger.info(f"[VISION NAV] ‚úÖ Extracted coordinates (format 3): ({x}, {y})")
                return (x, y)
            
            # Pattern 4: JSON format
            match = re.search(r'\{.*?"x":\s*(\d+).*?"y":\s*(\d+).*?\}', response, re.IGNORECASE | re.DOTALL)
            if match:
                x, y = int(match.group(1)), int(match.group(2))
                logger.info(f"[VISION NAV] ‚úÖ Extracted coordinates (format 4): ({x}, {y})")
                return (x, y)
            
            # Pattern 5: "approximately X pixels from the left/right" and "Y pixels from top"
            # Look for descriptions like "1400 pixels from the left edge"
            left_match = re.search(r'(\d+)\s*(?:pixels?)?\s*from\s*(?:the\s*)?left', response, re.IGNORECASE)
            right_match = re.search(r'(\d+)\s*(?:pixels?)?\s*from\s*(?:the\s*)?right', response, re.IGNORECASE)
            top_match = re.search(r'(\d+)\s*(?:pixels?)?\s*from\s*(?:the\s*)?top', response, re.IGNORECASE)
            
            if (left_match or right_match) and top_match:
                # Get screen dimensions
                screen_width, screen_height = pyautogui.size()
                
                if left_match:
                    x = int(left_match.group(1))
                elif right_match:
                    x = screen_width - int(right_match.group(1))
                
                y = int(top_match.group(1))
                logger.info(f"[VISION NAV] ‚úÖ Extracted coordinates (format 5): ({x}, {y})")
                return (x, y)
            
            # Pattern 6: Just two numbers in sequence (risky but try as last resort)
            numbers = re.findall(r'\b(\d{3,4})\b', response)  # 3-4 digit numbers (likely pixel coords)
            if len(numbers) >= 2:
                x, y = int(numbers[0]), int(numbers[1])
                # Sanity check: coordinates should be reasonable
                screen_width, screen_height = pyautogui.size()
                if 0 <= x <= screen_width and 0 <= y <= screen_height:
                    logger.info(f"[VISION NAV] ‚úÖ Extracted coordinates (format 6 - guessed): ({x}, {y})")
                    return (x, y)
            
            logger.warning(f"[VISION NAV] ‚ùå Could not extract coordinates from response.")
            logger.warning(f"[VISION NAV] Full response: {response[:500]}")
            return None
            
        except Exception as e:
            logger.error(f"[VISION NAV] Error extracting coordinates: {e}", exc_info=True)
            return None
    
    async def _click_at(self, x: int, y: int):
        """Click at specific coordinates"""
        try:
            logger.info(f"[VISION NAV] Clicking at ({x}, {y})")
            
            # Move to position
            pyautogui.moveTo(x, y, duration=self.config['mouse']['movement_speed'])
            
            # Brief pause
            await asyncio.sleep(0.1)
            
            # Click
            pyautogui.click(x, y, duration=self.config['mouse']['click_duration'])
            
            logger.debug(f"[VISION NAV] Click executed at ({x}, {y})")
            
        except Exception as e:
            logger.error(f"[VISION NAV] Click error: {e}")
            raise
    
    async def _click_control_center_heuristic(self) -> bool:
        """Fallback: Click Control Center using saved or heuristic position"""
        try:
            # Get screen dimensions
            screen_width, screen_height = pyautogui.size()

            logger.info(f"[VISION NAV] Screen dimensions: {screen_width}x{screen_height}")

            # Try to use saved position from config first
            cc_config = self.config.get('ui_elements', {}).get('control_center', {})

            if 'absolute_x' in cc_config and 'absolute_y' in cc_config:
                # Use saved position
                saved_x = cc_config['absolute_x']
                saved_y = cc_config['absolute_y']
                saved_screen_width = cc_config.get('screen_width', screen_width)

                # If screen resolution changed, adjust using offset
                if saved_screen_width != screen_width and 'offset_from_right' in cc_config:
                    offset = cc_config['offset_from_right']
                    x = screen_width - offset
                    y = saved_y
                    logger.info(f"[VISION NAV] Using adjusted position (screen resolution changed): ({x}, {y})")
                else:
                    x = saved_x
                    y = saved_y
                    logger.info(f"[VISION NAV] Using saved position from config: ({x}, {y})")

                await self._click_at(x, y)
                return True

            # Fallback: Use improved heuristic based on typical Control Center placement
            logger.info(f"[VISION NAV] No saved position, using improved heuristic...")
            logger.warning(f"[VISION NAV] üí° TIP: For perfect accuracy, let Claude Vision analyze your menu bar")

            # Control Center is typically about 150-200px from the right edge on most Macs
            # It's to the LEFT of the WiFi/Battery icons and time display
            # Try multiple likely positions in order of probability
            positions_to_try = [
                (screen_width - 180, 15, "180px from right (typical position)"),
                (screen_width - 160, 15, "160px from right"),
                (screen_width - 200, 15, "200px from right"),
                (screen_width - 150, 15, "150px from right"),
                (screen_width - 220, 15, "220px from right"),
            ]

            # Try the most likely position first
            x, y, description = positions_to_try[0]
            logger.info(f"[VISION NAV] Using heuristic: ({x}, {y}) - {description}")
            logger.info(f"[VISION NAV] This should click near the Control Center icon (two overlapping rectangles)")

            await self._click_at(x, y)
            return True

        except Exception as e:
            logger.error(f"[VISION NAV] Heuristic click failed: {e}")
            return False
    
    async def _click_screen_mirroring_ocr(self, screenshot: Image.Image) -> bool:
        """Use OCR to find and click Screen Mirroring"""
        try:
            # Use existing OCR infrastructure if available
            from vision.ocr_processor import OCRProcessor
            
            ocr = OCRProcessor()
            text_regions = await ocr.process_image(screenshot)
            
            # Look for "Screen Mirroring" or "Display"
            for region in text_regions:
                text = region.get('text', '').lower()
                if 'screen mirroring' in text or 'screen mirror' in text:
                    # Get bounding box
                    bbox = region.get('bbox')
                    if bbox:
                        # Calculate center
                        x = bbox[0] + bbox[2] // 2
                        y = bbox[1] + bbox[3] // 2
                        
                        await self._click_at(x, y)
                        return True
            
            return False
            
        except ImportError:
            logger.warning("[VISION NAV] OCR processor not available")
            return False
        except Exception as e:
            logger.error(f"[VISION NAV] OCR click failed: {e}")
            return False
    
    async def _click_display_ocr(self, screenshot: Image.Image, display_name: str) -> bool:
        """Use OCR to find and click display name"""
        try:
            from vision.ocr_processor import OCRProcessor
            
            ocr = OCRProcessor()
            text_regions = await ocr.process_image(screenshot)
            
            # Look for display name
            for region in text_regions:
                text = region.get('text', '')
                if display_name.lower() in text.lower():
                    bbox = region.get('bbox')
                    if bbox:
                        x = bbox[0] + bbox[2] // 2
                        y = bbox[1] + bbox[3] // 2
                        
                        await self._click_at(x, y)
                        return True
            
            return False
            
        except Exception as e:
            logger.error(f"[VISION NAV] OCR display click failed: {e}")
            return False
    
    def get_stats(self) -> Dict[str, Any]:
        """Get navigation statistics"""
        success_rate = 0.0
        if self.stats['total_navigations'] > 0:
            success_rate = (self.stats['successful'] / self.stats['total_navigations']) * 100
        
        return {
            'total_navigations': self.stats['total_navigations'],
            'successful': self.stats['successful'],
            'failed': self.stats['failed'],
            'success_rate': round(success_rate, 2),
            'avg_duration': round(self.stats['avg_duration'], 2),
            'vision_analyzer_connected': self.vision_analyzer is not None
        }
    
    def get_status(self) -> Dict[str, Any]:
        """Get navigator status"""
        return {
            'initialized': True,
            'config_loaded': self.config is not None,
            'vision_connected': self.vision_analyzer is not None,
            'screenshots_dir': str(self.screenshots_dir),
            'stats': self.get_stats()
        }


# Singleton instance
_navigator_instance: Optional[VisionUINavigator] = None


def get_vision_navigator(config_path: Optional[str] = None) -> VisionUINavigator:
    """Get singleton vision navigator instance"""
    global _navigator_instance
    if _navigator_instance is None:
        _navigator_instance = VisionUINavigator(config_path)
    return _navigator_instance


if __name__ == "__main__":
    # Test the navigator
    async def test():
        logging.basicConfig(level=logging.INFO)
        
        navigator = get_vision_navigator()
        
        print("\n" + "="*60)
        print("Vision UI Navigator Test")
        print("="*60)
        
        # Get status
        print("\n1. Navigator Status:")
        status = navigator.get_status()
        for key, value in status.items():
            print(f"   {key}: {value}")
        
        # Test connection
        print("\n2. Testing connection to Living Room TV...")
        print("   (This will actually attempt to connect!)")
        
        result = await navigator.connect_to_display("Living Room TV")
        
        print(f"\n3. Result:")
        print(f"   Success: {result.success}")
        print(f"   Message: {result.message}")
        print(f"   Duration: {result.duration:.2f}s")
        print(f"   Steps: {result.steps_completed}")
        
        # Get stats
        print("\n4. Statistics:")
        stats = navigator.get_stats()
        for key, value in stats.items():
            print(f"   {key}: {value}")
        
        print("\n" + "="*60)
    
    asyncio.run(test())
