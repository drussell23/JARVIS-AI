"""
Claude Vision-enabled Chatbot for JARVIS
Extends the basic Claude chatbot with vision capabilities for screen analysis
"""

# Fix import path for vision modules
import sys
import os
backend_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
if backend_path not in sys.path:
    sys.path.insert(0, backend_path)

import logging
import asyncio
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timedelta
import json
import base64
import io
from PIL import Image
import numpy as np
import hashlib

# Platform and system detection
import platform
import subprocess
import tempfile
import re
from pathlib import Path
from dataclasses import dataclass, field
from enum import Enum

logger = logging.getLogger(__name__)

# Check if required packages are available
try:
    import anthropic
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    logger.warning("Anthropic package not installed. Install with: pip install anthropic")

try:
    import pyautogui
    SCREENSHOT_AVAILABLE = True
except (ImportError, AttributeError) as e:
    SCREENSHOT_AVAILABLE = False
    logger.warning(f"PyAutoGUI not available: {e}. Screenshot fallback disabled.")

try:
    from vision.claude_vision_analyzer_main import ClaudeVisionAnalyzer
    VISION_ANALYZER_AVAILABLE = True
except ImportError:
    VISION_ANALYZER_AVAILABLE = False
    logger.warning("Claude vision analyzer not available")


class ClaudeVisionChatbot:
    """
    Vision-enabled Claude chatbot that can analyze screen content
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        model: str = "claude-3-5-sonnet-20241022",  # Vision-capable model
        max_tokens: int = 1024,
        temperature: float = 0.7,
        system_prompt: Optional[str] = None,
        vision_analyzer: Optional[Any] = None  # Allow passing existing analyzer
    ):
        """Initialize Claude vision chatbot"""
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        if not self.api_key:
            raise ValueError(
                "Anthropic API key required. Set ANTHROPIC_API_KEY environment variable "
                "or pass api_key parameter"
            )
            
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature
        
        # Dynamic JARVIS system prompt with real-time context
        self._initialize_dynamic_system_prompt(system_prompt)
        
        # Initialize client
        if ANTHROPIC_AVAILABLE:
            self.client = anthropic.Anthropic(api_key=self.api_key)
        else:
            self.client = None
            
        # Use provided vision analyzer or initialize new one
        if vision_analyzer:
            # Use the provided analyzer (e.g., from app.state)
            self.vision_analyzer = vision_analyzer
            logger.info("Using provided vision analyzer instance")
        elif VISION_ANALYZER_AVAILABLE and self.api_key:
            # Create new analyzer with real-time monitoring by default
            self.vision_analyzer = ClaudeVisionAnalyzer(
                self.api_key, 
                enable_realtime=True
            )
            logger.info("Initialized new Claude Vision Analyzer with real-time monitoring capabilities")
        else:
            self.vision_analyzer = None
            logger.warning("Vision analyzer not available - real-time monitoring disabled")
            
        # Conversation history
        self.conversation_history: List[Dict[str, str]] = []
        self.max_history_length = 10
        
        # Monitoring state
        self._monitoring_active = False
        self._capture_method = 'unknown'
        
        # Screenshot cache (cache for 5 seconds to handle repeated requests)
        self._screenshot_cache = None
        self._screenshot_cache_time = None
        self._screenshot_cache_duration = timedelta(seconds=5)
        
        # Dynamic vision detection configuration
        self._initialize_vision_detection_system()
        
        # Platform-specific configuration
        self._platform = platform.system().lower()
        self._capture_methods = self._initialize_capture_methods()
        
        # Dynamic cache configuration
        self._cache_config = self._get_cache_configuration()
        
    def is_vision_command(self, user_input: str) -> bool:
        """Enhanced vision command detection with intent analysis"""
        input_lower = user_input.lower().strip()
        
        # Quick keyword pre-check for performance
        if not any(keyword in input_lower for keyword in self._vision_keywords):
            return False
            
        # Check regex patterns for more accurate detection
        for pattern in self._compiled_vision_patterns:
            if pattern.search(input_lower):
                # Analyze intent for later use
                self._last_vision_intent = self._analyze_vision_intent(input_lower)
                return True
                
        # Fuzzy matching for typos and variations
        words = input_lower.split()
        for word in words:
            for keyword in self._vision_keywords:
                # Allow for typos (edit distance)
                if self._similar_words(word, keyword, threshold=0.8):
                    self._last_vision_intent = self._analyze_vision_intent(input_lower)
                    return True
                    
        return False
        
    async def capture_screenshot(self) -> Optional[Image.Image]:
        """Enhanced screenshot capture with multiple methods and smart caching"""
        # Generate cache key based on intent
        cache_key = self._generate_cache_key()
        
        # Check intelligent cache
        cached_screenshot = await self._get_cached_screenshot(cache_key)
        if cached_screenshot:
            logger.info(f"Using cached screenshot (key: {cache_key})")
            return cached_screenshot
            
        # Try capture methods in platform-specific order
        screenshot = None
        used_method = None
        errors = []
        
        for method_info in self._capture_methods:
            method_name = method_info['name']
            method_func = method_info['function']
            
            try:
                logger.debug(f"Attempting screenshot capture via {method_name}")
                screenshot = await method_func()
                
                if screenshot:
                    # Validate the screenshot
                    if isinstance(screenshot, Image.Image):
                        width, height = screenshot.size
                        if width > 0 and height > 0:
                            used_method = method_name
                            logger.info(f"Successfully captured screenshot using {method_name}: {width}x{height}")
                            break
                        else:
                            logger.warning(f"{method_name} returned invalid image size: {width}x{height}")
                            errors.append(f"{method_name}: Invalid image size")
                    else:
                        logger.warning(f"{method_name} returned non-Image object: {type(screenshot)}")
                        errors.append(f"{method_name}: Invalid type")
                        
            except Exception as e:
                error_msg = f"{method_name} capture failed: {type(e).__name__}: {str(e)}"
                logger.warning(error_msg)
                errors.append(error_msg)
                continue
                
        if not screenshot:
            logger.error(f"All screenshot capture methods failed. Errors: {errors}")
            return None
            
        # Post-process screenshot based on intent
        if hasattr(self, '_last_vision_intent'):
            screenshot = await self._optimize_screenshot(screenshot, self._last_vision_intent)
            
        # Cache the screenshot
        await self._cache_screenshot(cache_key, screenshot, used_method)
        
        return screenshot
            
    async def analyze_screen_with_vision(self, user_input: str) -> str:
        """Enhanced screen analysis with dynamic optimization"""
        try:
            total_start = datetime.now()
            
            # Capture screenshot
            capture_start = datetime.now()
            screenshot = await self.capture_screenshot()
            capture_time = (datetime.now() - capture_start).total_seconds()
            logger.info(f"Screenshot capture took {capture_time:.2f}s")
            
            if not screenshot:
                return "I apologize, sir, but I'm unable to capture your screen at the moment. Please ensure screen recording permissions are enabled in System Preferences > Security & Privacy > Privacy > Screen Recording."
                
            # Dynamic image optimization based on intent
            encode_start = datetime.now()
            screenshot = await self._prepare_screenshot_for_api(screenshot, user_input)
            
            # Convert to base64 for Claude API with optimization
            buffer = io.BytesIO()
            # Convert RGBA to RGB if needed for JPEG
            if screenshot.mode == 'RGBA':
                # Create a white background
                rgb_image = Image.new('RGB', screenshot.size, (255, 255, 255))
                rgb_image.paste(screenshot, mask=screenshot.split()[3])  # Use alpha channel as mask
                screenshot = rgb_image
            # Dynamic format and quality based on intent
            format_config = self._get_image_format_config(user_input)
            screenshot.save(
                buffer, 
                format=format_config['format'], 
                quality=format_config['quality'], 
                optimize=format_config['optimize']
            )
            image_base64 = base64.b64encode(buffer.getvalue()).decode()
            encode_time = (datetime.now() - encode_start).total_seconds()
            logger.info(f"Image encoding took {encode_time:.2f}s (format: {format_config['format']}, quality: {format_config['quality']})")
            
            # Build the vision message
            messages = self._build_messages_with_vision(user_input, image_base64)
            
            # Make API call with vision
            api_start = datetime.now()
            
            # Use a faster model for quick vision checks if available
            vision_model = self.model
            if "can you see" in user_input.lower() and "claude-3-haiku" in self.model:
                # For simple "can you see" queries, we can use the same model
                pass
            
            # Dynamic prompt generation based on query analysis
            system_prompt = self._generate_vision_system_prompt(user_input)
            
            # Dynamic API configuration based on intent
            api_config = self._get_vision_api_config(user_input)
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=api_config['model'],
                max_tokens=api_config['max_tokens'],
                temperature=api_config['temperature'],
                system=system_prompt,
                messages=messages
            )
            
            # Extract response
            ai_response = response.content[0].text
            
            # Log performance
            api_time = (datetime.now() - api_start).total_seconds()
            total_time = (datetime.now() - total_start).total_seconds()
            logger.info(f"Claude API call took {api_time:.2f}s")
            logger.info(f"Total vision processing took {total_time:.2f}s")
            
            # Update conversation history
            self._update_history(user_input, ai_response)
            
            return ai_response
            
        except anthropic.APIError as e:
            logger.error(f"Claude API error: {e}")
            return f"I encountered an API error while analyzing your screen: {str(e)}"
        except Exception as e:
            logger.error(f"Error in screen analysis: {e}")
            return f"I apologize, sir, but I encountered an error analyzing your screen: {str(e)}"
            
    def _initialize_dynamic_system_prompt(self, custom_prompt: Optional[str] = None):
        """Initialize system prompt with dynamic context"""
        # Get dynamic time context
        current_datetime = datetime.now().strftime("%A, %B %d, %Y at %I:%M %p")
        timezone = self._get_timezone_name()
        
        # Build dynamic prompt components
        base_components = [
            "You are JARVIS, an intelligent AI assistant inspired by Tony Stark's AI from Iron Man.",
            "You have advanced vision capabilities and can see and analyze the user's screen when asked.",
            "You are helpful, witty, and highly capable.",
            "You speak with a refined, professional tone while being personable and occasionally adding subtle humor.",
            "When analyzing screens, you provide detailed, accurate descriptions and helpful insights about what you observe.",
            "You excel at understanding context and providing insightful, well-structured responses."
        ]
        
        # Add dynamic context
        context_components = [
            f"Current date and time: {current_datetime}",
            f"Timezone: {timezone}" if timezone else None,
            f"Platform: {platform.system()} {platform.release()}"
        ]
        
        # Filter out None values and join
        all_components = base_components + [c for c in context_components if c]
        self.system_prompt = custom_prompt or " ".join(all_components)
    
    def _initialize_vision_detection_system(self):
        """Initialize dynamic vision pattern detection"""
        # Core vision keywords for quick checks
        self._vision_keywords = {
            'screen', 'see', 'look', 'view', 'show', 'display', 'vision',
            'visual', 'analyze', 'examine', 'check', 'monitor', 'desktop',
            'window', 'describe', 'watch', 'observe', 'inspect'
        }
        
        # Build regex patterns dynamically
        self._compiled_vision_patterns = self._compile_vision_patterns()
        
    def _compile_vision_patterns(self) -> List[re.Pattern]:
        """Compile vision detection patterns dynamically"""
        patterns = []
        
        # Action + target patterns
        action_words = ['see', 'look', 'view', 'show', 'analyze', 'check', 'examine', 'describe']
        target_words = ['screen', 'display', 'monitor', 'desktop', 'window']
        
        for action in action_words:
            for target in target_words:
                patterns.extend([
                    rf'\b{action}\s+(?:my\s+|the\s+)?{target}\b',
                    rf'\bcan\s+you\s+{action}\s+(?:my\s+|the\s+)?{target}\b',
                    rf'\b{action}\s+what(?:\'s|\s+is)\s+on\s+(?:my\s+|the\s+)?{target}\b'
                ])
        
        # Question patterns
        patterns.extend([
            r'\bwhat\s+do\s+you\s+see\b',
            r'\bwhat(?:\'s|\s+is)\s+on\s+(?:my\s+)?screen\b',
            r'\btell\s+me\s+what\s+you\s+see\b',
            r'\banalyze\s+this\b',
            r'\blook\s+at\s+this\b',
            r'\bscreen\s*shot\b',
            r'\bvisual\s+analysis\b'
        ])
        
        # Compile all patterns with case-insensitive flag
        return [re.compile(p, re.IGNORECASE) for p in patterns]
    
    def _initialize_capture_methods(self) -> List[Dict[str, Any]]:
        """Initialize platform-specific capture methods"""
        methods = []
        
        # PyAutoGUI (cross-platform)
        if SCREENSHOT_AVAILABLE:
            methods.append({
                'name': 'pyautogui',
                'function': self._capture_pyautogui,
                'priority': 1
            })
        
        # Platform-specific methods
        if self._platform == 'darwin':  # macOS
            methods.extend([
                {
                    'name': 'native_macos',
                    'function': self._capture_native_macos,
                    'priority': 2
                },
                {
                    'name': 'screencapture_cmd',
                    'function': self._capture_screencapture_cmd,
                    'priority': 3
                }
            ])
        elif self._platform == 'win32':  # Windows
            methods.append({
                'name': 'windows_capture',
                'function': self._capture_windows,
                'priority': 2
            })
        elif 'linux' in self._platform:  # Linux
            methods.append({
                'name': 'linux_capture',
                'function': self._capture_linux,
                'priority': 2
            })
        
        # Vision analyzer as fallback
        if self.vision_analyzer:
            methods.append({
                'name': 'vision_analyzer',
                'function': self._capture_vision_analyzer,
                'priority': 99
            })
        
        # Sort by priority
        return sorted(methods, key=lambda x: x['priority'])
    
    def _get_cache_configuration(self) -> Dict[str, Any]:
        """Get dynamic cache configuration"""
        return {
            'duration': timedelta(seconds=5),
            'max_size': 10,
            'strategy': 'lru',  # Least Recently Used
            'hash_method': 'md5'
        }
    
    def _analyze_vision_intent(self, text: str) -> Dict[str, Any]:
        """Analyze the intent behind a vision query"""
        intent = {
            'urgency': 'normal',
            'detail_level': 'standard',
            'focus_areas': [],
            'query_type': 'general',
            'requires_interaction': False
        }
        
        # Detect urgency
        if any(word in text for word in ['urgent', 'quick', 'fast', 'immediately', 'asap']):
            intent['urgency'] = 'high'
        
        # Detect detail level
        if any(word in text for word in ['detail', 'comprehensive', 'thorough', 'everything', 'full']):
            intent['detail_level'] = 'high'
        elif any(word in text for word in ['brief', 'summary', 'quick', 'glance']):
            intent['detail_level'] = 'low'
        
        # Detect focus areas
        if any(word in text for word in ['error', 'bug', 'issue', 'problem']):
            intent['focus_areas'].append('errors')
        if any(word in text for word in ['code', 'program', 'script']):
            intent['focus_areas'].append('code')
        if any(word in text for word in ['text', 'document', 'content']):
            intent['focus_areas'].append('text')
        
        # Detect query type
        if 'can you see' in text.lower():
            intent['query_type'] = 'confirmation'
        elif any(word in text for word in ['analyze', 'examine']):
            intent['query_type'] = 'analysis'
        
        return intent
    
    def _similar_words(self, word1: str, word2: str, threshold: float = 0.8) -> bool:
        """Check if two words are similar (for typo detection)"""
        # Simple similarity check - can be enhanced with edit distance
        if word1 == word2:
            return True
        
        # Check if one is substring of other
        if len(word1) > 3 and len(word2) > 3:
            if word1 in word2 or word2 in word1:
                return True
        
        # Basic edit distance approximation
        if abs(len(word1) - len(word2)) > 2:
            return False
        
        matches = sum(1 for a, b in zip(word1, word2) if a == b)
        similarity = matches / max(len(word1), len(word2))
        
        return similarity >= threshold
    
    async def _capture_pyautogui(self) -> Optional[Image.Image]:
        """Capture using PyAutoGUI"""
        if not SCREENSHOT_AVAILABLE:
            return None
        try:
            return await asyncio.to_thread(pyautogui.screenshot)
        except Exception as e:
            raise Exception(f"PyAutoGUI capture failed: {e}")
    
    async def _capture_native_macos(self) -> Optional[Image.Image]:
        """Capture using native macOS methods"""
        try:
            from vision.screen_capture_module import capture_screen_native
            screenshot_path = capture_screen_native()
            if screenshot_path:
                return Image.open(screenshot_path)
        except:
            pass
        return None
    
    async def _capture_screencapture_cmd(self) -> Optional[Image.Image]:
        """Capture using macOS screencapture command"""
        try:
            with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                cmd = ['screencapture', '-x']
                # Add cursor if needed based on intent
                if hasattr(self, '_last_vision_intent') and self._last_vision_intent.get('show_cursor'):
                    cmd.append('-C')
                cmd.append(tmp.name)
                
                result = await asyncio.create_subprocess_exec(
                    *cmd,
                    stdout=asyncio.subprocess.PIPE,
                    stderr=asyncio.subprocess.PIPE
                )
                await result.communicate()
                
                if result.returncode == 0:
                    return Image.open(tmp.name)
        except:
            pass
        return None
    
    async def _capture_windows(self) -> Optional[Image.Image]:
        """Capture on Windows"""
        try:
            from PIL import ImageGrab
            return await asyncio.to_thread(ImageGrab.grab)
        except:
            return None
    
    async def _capture_linux(self) -> Optional[Image.Image]:
        """Capture on Linux"""
        commands = [
            ['gnome-screenshot', '-f'],
            ['scrot'],
            ['import', '-window', 'root']
        ]
        
        for cmd_template in commands:
            try:
                with tempfile.NamedTemporaryFile(suffix='.png', delete=False) as tmp:
                    cmd = cmd_template + [tmp.name]
                    result = await asyncio.create_subprocess_exec(
                        *cmd,
                        stdout=asyncio.subprocess.PIPE,
                        stderr=asyncio.subprocess.PIPE
                    )
                    await result.communicate()
                    
                    if result.returncode == 0:
                        return Image.open(tmp.name)
            except:
                continue
        return None
    
    async def _capture_vision_analyzer(self) -> Optional[Image.Image]:
        """Capture using vision analyzer"""
        if self.vision_analyzer and hasattr(self.vision_analyzer, 'capture_screen'):
            return await self.vision_analyzer.capture_screen()
        return None
    
    def _generate_cache_key(self) -> str:
        """Generate intelligent cache key"""
        components = [
            str(datetime.now().timestamp()),
            getattr(self, '_last_vision_intent', {}).get('query_type', 'general')
        ]
        
        key_string = "_".join(components)
        return hashlib.md5(key_string.encode()).hexdigest()[:16]
    
    async def _get_cached_screenshot(self, cache_key: str) -> Optional[Image.Image]:
        """Get screenshot from intelligent cache"""
        if not hasattr(self, '_screenshot_cache_store'):
            self._screenshot_cache_store = {}
        
        cached = self._screenshot_cache_store.get(cache_key)
        if cached:
            timestamp, screenshot, metadata = cached
            cache_duration = self._cache_config['duration']
            
            # Check if cache is still valid
            if datetime.now() - timestamp < cache_duration:
                # Extend cache for frequently accessed items
                if metadata.get('access_count', 0) > 3:
                    cache_duration *= 2
                
                metadata['access_count'] = metadata.get('access_count', 0) + 1
                return screenshot
        
        return None
    
    async def _cache_screenshot(self, cache_key: str, screenshot: Image.Image, method: str):
        """Cache screenshot with metadata"""
        if not hasattr(self, '_screenshot_cache_store'):
            self._screenshot_cache_store = {}
        
        # Implement LRU cache
        max_size = self._cache_config['max_size']
        if len(self._screenshot_cache_store) >= max_size:
            # Remove least recently used
            oldest_key = min(self._screenshot_cache_store.keys(), 
                           key=lambda k: self._screenshot_cache_store[k][2].get('last_access', datetime.min))
            del self._screenshot_cache_store[oldest_key]
        
        self._screenshot_cache_store[cache_key] = (
            datetime.now(),
            screenshot,
            {
                'method': method,
                'access_count': 0,
                'last_access': datetime.now()
            }
        )
    
    async def _optimize_screenshot(self, screenshot: Image.Image, intent: Dict[str, Any]) -> Image.Image:
        """Optimize screenshot based on intent analysis"""
        # Dynamic resizing based on detail level
        if intent.get('detail_level') == 'low':
            max_dimension = 1280
        elif intent.get('detail_level') == 'high':
            max_dimension = 2560
        else:
            max_dimension = 1920
        
        # Resize if needed
        if screenshot.width > max_dimension or screenshot.height > max_dimension:
            ratio = min(max_dimension / screenshot.width, max_dimension / screenshot.height)
            new_size = (int(screenshot.width * ratio), int(screenshot.height * ratio))
            screenshot = screenshot.resize(new_size, Image.Resampling.LANCZOS)
            logger.info(f"Optimized screenshot to {new_size} based on intent")
        
        return screenshot
    
    async def _prepare_screenshot_for_api(self, screenshot: Image.Image, user_input: str) -> Image.Image:
        """Prepare screenshot for API submission"""
        intent = getattr(self, '_last_vision_intent', {})
        
        # Apply optimizations
        screenshot = await self._optimize_screenshot(screenshot, intent)
        
        # Convert RGBA to RGB if needed
        if screenshot.mode == 'RGBA':
            rgb_image = Image.new('RGB', screenshot.size, (255, 255, 255))
            rgb_image.paste(screenshot, mask=screenshot.split()[3])
            screenshot = rgb_image
        
        return screenshot
    
    def _get_image_format_config(self, user_input: str) -> Dict[str, Any]:
        """Get dynamic image format configuration"""
        intent = getattr(self, '_last_vision_intent', {})
        
        # High quality for detailed analysis
        if intent.get('detail_level') == 'high':
            return {
                'format': 'PNG',
                'quality': 95,
                'optimize': True
            }
        # Fast processing for quick checks
        elif intent.get('urgency') == 'high' or intent.get('query_type') == 'confirmation':
            return {
                'format': 'JPEG',
                'quality': 70,
                'optimize': True
            }
        # Standard balanced approach
        else:
            return {
                'format': 'JPEG',
                'quality': 85,
                'optimize': True
            }
    
    def _generate_vision_system_prompt(self, user_input: str) -> str:
        """Generate dynamic system prompt for vision queries"""
        current_datetime = datetime.now().strftime("%A, %B %d, %Y at %I:%M %p")
        user_lower = user_input.lower()
        
        # Base JARVIS prompt
        base = (
            "You are JARVIS, Tony Stark's AI assistant with advanced vision capabilities. "
            f"Current date and time: {current_datetime}. "
            "When analyzing screens, be specific and precise - read actual values, not generic descriptions. "
        )
        
        # Add specific instructions based on query type
        if any(word in user_lower for word in ['battery', 'power', 'charge']):
            specific = (
                "The user is asking about battery status. "
                "Always report the EXACT percentage number visible on screen. "
                "Never give vague responses like 'the battery appears to be...' - give specific values."
            )
        elif any(word in user_lower for word in ['time', 'clock']):
            specific = (
                "The user is asking about the time. "
                "Report the EXACT time shown on the screen's clock. "
                "Be precise with the format (e.g., '2:34 PM' or '14:34')."
            )
        elif 'status bar' in user_lower or 'menu bar' in user_lower:
            specific = (
                "The user wants to know about the status/menu bar. "
                "List every visible element with its specific value or state. "
                "Include time, battery percentage, network status, and all visible icons."
            )
        else:
            specific = (
                "Provide specific, actionable information about what you see. "
                "Include exact values for UI elements like time, battery, notifications. "
                "Name specific applications and describe actual content, not generic observations."
            )
        
        # Combine components
        return base + specific
    
    def _get_vision_api_config(self, user_input: str) -> Dict[str, Any]:
        """Get dynamic API configuration for vision requests"""
        intent = getattr(self, '_last_vision_intent', {})
        
        # Model selection based on requirements
        if intent.get('urgency') == 'high' and 'haiku' in self.model.lower():
            model = self.model  # Use faster model if available
        else:
            model = self.model
        
        # Token limits based on detail level
        if intent.get('detail_level') == 'high':
            max_tokens = 2048
        elif intent.get('query_type') == 'confirmation':
            max_tokens = 256
        else:
            max_tokens = self.max_tokens
        
        # Temperature based on task type
        if intent.get('query_type') == 'analysis':
            temperature = 0.3  # More focused
        else:
            temperature = self.temperature
        
        return {
            'model': model,
            'max_tokens': max_tokens,
            'temperature': temperature
        }
    
    def _get_timezone_name(self) -> Optional[str]:
        """Get system timezone name"""
        try:
            if self._platform == 'darwin':
                result = subprocess.run(['systemsetup', '-gettimezone'], 
                                      capture_output=True, text=True)
                if result.returncode == 0:
                    output = result.stdout.strip()
                    if "Time Zone:" in output:
                        return output.split("Time Zone:")[1].strip()
            elif os.path.exists('/etc/timezone'):
                with open('/etc/timezone', 'r') as f:
                    return f.read().strip()
        except:
            pass
        return None
            
    def _build_messages_with_vision(self, user_input: str, image_base64: str) -> List[Dict[str, Any]]:
        """Build message list for Claude API including vision content"""
        messages = []
        
        # Add conversation history (last 3 exchanges for context)
        for entry in self.conversation_history[-3:]:
            messages.append({"role": "user", "content": entry["user"]})
            messages.append({"role": "assistant", "content": entry["assistant"]})
            
        # Enhance the prompt based on what the user is asking
        enhanced_prompt = self._enhance_vision_prompt(user_input)
        
        # Add current user input with image
        messages.append({
            "role": "user",
            "content": [
                {
                    "type": "image",
                    "source": {
                        "type": "base64",
                        "media_type": "image/jpeg",
                        "data": image_base64
                    }
                },
                {
                    "type": "text",
                    "text": enhanced_prompt
                }
            ]
        })
        
        return messages
    
    def _enhance_vision_prompt(self, user_input: str) -> str:
        """Enhance prompt for specific UI element detection using Claude's pure vision intelligence"""
        user_lower = user_input.lower()
        
        # Core vision intelligence framework
        vision_intelligence = (
            "Use your advanced vision intelligence to understand the full context of what you see:\n"
            "- DISTINGUISH between system UI and application content (screenshots, mockups, videos)\n"
            "- UNDERSTAND window layering and what's in foreground vs background\n"
            "- EXPRESS CONFIDENCE: 'I'm certain', 'I believe', or 'I'm unsure' when appropriate\n"
            "- ACKNOWLEDGE LIMITATIONS: If something is hidden, obscured, or ambiguous\n\n"
        )
        
        # Check for battery queries
        if any(word in user_lower for word in ['battery', 'power', 'charge', 'charging']):
            return (f"{user_input}\n\n{vision_intelligence}"
                   "SPECIFIC TASK: Report the SYSTEM battery level.\n"
                   "- Look in the system status area (top-right macOS, bottom-right Windows)\n"
                   "- If you see multiple battery indicators, identify which is the actual system battery\n"
                   "- Report format: 'Your battery is at X%' or 'I see multiple battery indicators...'\n"
                   "- If battery is hidden or obscured, say so explicitly")
        
        # Check for time queries  
        elif any(word in user_lower for word in ['time', 'clock', 'hour']):
            return (f"{user_input}\n\n{vision_intelligence}"
                   "SPECIFIC TASK: Report the current SYSTEM time.\n"
                   "- Find the system clock (not times in videos, code, or screenshots)\n"
                   "- If multiple times are visible, use visual cues to identify the system time\n"
                   "- Report format: 'The time is X:XX PM'\n"
                   "- Express confidence if there's ambiguity")
        
        # Check for status bar queries
        elif 'status bar' in user_lower or 'menu bar' in user_lower:
            return (f"{user_input}\n\n{vision_intelligence}"
                   "SPECIFIC TASK: Analyze the system status/menu bar.\n"
                   "- Focus on the actual system bar, not app toolbars\n"
                   "- Report all visible elements with exact values\n"
                   "- Note which parts might be hidden by other windows\n"
                   "- List from left to right or by importance")
        
        # Check for app-specific queries
        app_keywords = ['slack', 'discord', 'chrome', 'safari', 'vscode', 'terminal', 'finder', 'mail']
        mentioned_app = None
        for app in app_keywords:
            if app in user_lower:
                mentioned_app = app
                break
        
        if mentioned_app:
            return (f"{user_input}\n\n{vision_intelligence}"
                   f"SPECIFIC TASK: User is asking about {mentioned_app.title()}.\n"
                   f"- Determine if {mentioned_app.title()} is visible, partially visible, or not visible\n"
                   f"- If not visible, explain why (minimized, different desktop, not running)\n"
                   f"- Don't assume or hallucinate its presence")
        
        # Check for general screen queries
        elif any(phrase in user_lower for phrase in ['what do you see', "what's on", 'can you see', 'describe']):
            return (f"{user_input}\n\n{vision_intelligence}"
                   "COMPREHENSIVE SCREEN ANALYSIS:\n"
                   "1) Identify all visible windows and their layering order\n"
                   "2) Note which window appears to be active/in focus\n"
                   "3) Report system UI elements with exact values\n"
                   "4) Describe spatial relationships and partial occlusions\n"
                   "5) Mention any dynamic content or loading states\n"
                   "6) If asked about something specific that's not visible, say so")
        
        # Multi-monitor awareness
        if any(phrase in user_lower for phrase in ['other monitor', 'second screen', 'external monitor']):
            return (f"{user_input}\n\n{vision_intelligence}"
                   "NOTE: User may be asking about a different monitor.\n"
                   "- You can only see what's in this screenshot\n"
                   "- If they need another monitor analyzed, suggest they specify which screen")
        
        # Default - return with general intelligence enhancement
        return f"{user_input}\n\n{vision_intelligence}Provide specific, intelligent analysis based on what you actually see."
        
    async def _is_monitoring_command(self, user_input: str) -> bool:
        """Check if this is a continuous monitoring command"""
        monitoring_keywords = [
            'monitor', 'monitoring', 'watch', 'watching', 'track', 'tracking',
            'continuous', 'continuously', 'real-time', 'realtime', 'actively',
            'surveillance', 'observe', 'observing', 'stream', 'streaming'
        ]
        
        screen_keywords = ['screen', 'display', 'desktop', 'workspace', 'monitor']
        
        text_lower = user_input.lower()
        has_monitoring = any(keyword in text_lower for keyword in monitoring_keywords)
        has_screen = any(keyword in text_lower for keyword in screen_keywords)
        
        return has_monitoring and has_screen
        
    async def _handle_monitoring_command(self, user_input: str) -> str:
        """Handle continuous monitoring commands"""
        text_lower = user_input.lower()
        logger.info(f"[MONITORING] Processing monitoring command: {user_input}")
        
        # Check if we have the enhanced vision analyzer with video streaming
        if self.vision_analyzer is not None:
            logger.info("[MONITORING] Vision analyzer is available, attempting to handle monitoring")
            try:
                # Use the already initialized vision analyzer
                # Handle different monitoring commands
                if any(word in text_lower for word in ['start', 'enable', 'activate', 'begin', 'turn on']):
                    logger.info("[MONITORING] Starting video streaming...")
                    logger.info(f"[MONITORING] Vision analyzer available: {self.vision_analyzer is not None}")
                    logger.info(f"[MONITORING] Vision analyzer ID: {id(self.vision_analyzer) if self.vision_analyzer else None}")
                    
                    # Check video streaming module before starting
                    if hasattr(self.vision_analyzer, 'get_video_streaming'):
                        vs = await self.vision_analyzer.get_video_streaming()
                        logger.info(f"[MONITORING] Video streaming module exists: {vs is not None}")
                        if vs:
                            logger.info(f"[MONITORING] Video streaming already capturing: {vs.is_capturing}")
                    
                    # Start video streaming
                    try:
                        logger.info(f"[MONITORING] Vision analyzer type: {type(self.vision_analyzer).__name__}")
                        logger.info(f"[MONITORING] Vision analyzer has start_video_streaming: {hasattr(self.vision_analyzer, 'start_video_streaming')}")
                        logger.info(f"[MONITORING] Vision analyzer config: enable_video_streaming={self.vision_analyzer.config.enable_video_streaming}")
                        result = await self.vision_analyzer.start_video_streaming()
                        logger.info(f"[MONITORING] Video streaming result: {result}")
                        logger.info(f"[MONITORING] Result success: {result.get('success')}")
                        
                        if result.get('success'):
                            # Update monitoring state
                            self._monitoring_active = True
                            capture_method = result.get('metrics', {}).get('capture_method', 'unknown')
                            
                            # IMPORTANT: Return the proper response about video capture activation
                            if capture_method == 'macos_native':
                                return "I have successfully activated native macOS video capturing for monitoring your screen. The purple recording indicator should now be visible in your menu bar, confirming that screen recording is active. I'm capturing at 30 FPS and will continuously monitor for any changes or important events on your screen."
                            elif capture_method == 'swift_native':
                                return "I have successfully activated Swift-based macOS video capturing for monitoring your screen. The purple recording indicator should now be visible in your menu bar, confirming that screen recording is active. I'm capturing at 30 FPS with enhanced permission handling and will continuously monitor for any changes or important events on your screen."
                            elif capture_method == 'direct_swift':
                                return "I have successfully activated direct Swift video capturing for monitoring your screen. The purple recording indicator should now be visible in your menu bar, confirming that screen recording is active. I'm monitoring continuously at 30 FPS and will watch for any changes or important events on your screen until you tell me to stop."
                            elif capture_method == 'purple_indicator':
                                return "I have successfully started monitoring your screen. The purple recording indicator should now be visible in your menu bar, confirming that screen recording is active. I'm capturing your screen at 30 FPS and will continuously monitor for any changes or important events until you tell me to stop."
                            else:
                                return f"I've started monitoring your screen using {capture_method} capture mode at 30 FPS. I'll continuously watch for any changes or important events on your screen."
                        else:
                            error_msg = result.get('error', 'Unknown error')
                            logger.error(f"[MONITORING] Failed to start video streaming: {error_msg}")
                            return f"I encountered an issue starting video streaming: {error_msg}. Please check that screen recording permissions are enabled in System Preferences."
                            
                    except Exception as e:
                        logger.error(f"[MONITORING] Exception starting video streaming: {e}")
                        return f"I encountered an error starting video monitoring: {str(e)}. Please ensure screen recording permissions are enabled."
                        
                elif any(word in text_lower for word in ['stop', 'disable', 'deactivate', 'end', 'turn off']):
                    # Stop video streaming
                    result = await self.vision_analyzer.stop_video_streaming()
                    if result.get('success'):
                        self._monitoring_active = False
                        self._capture_method = 'unknown'
                        return "I've stopped monitoring your screen. The video streaming has been disabled and the recording indicator should have disappeared."
                    else:
                        self._monitoring_active = False
                        self._capture_method = 'unknown'
                        return "The screen monitoring appears to be already stopped."
                        
                else:
                    # Generic monitoring request - start monitoring and describe
                    result = await self.vision_analyzer.start_video_streaming()
                    if result.get('success'):
                        # Update monitoring state
                        self._monitoring_active = True
                        self._capture_method = result.get('metrics', {}).get('capture_method', 'unknown')
                        
                        # Analyze for 5 seconds
                        analysis_result = await self.vision_analyzer.analyze_video_stream(
                            "Monitor the screen and describe any changes or important elements you see.",
                            duration_seconds=5.0
                        )
                        
                        if analysis_result.get('success'):
                            frames_analyzed = analysis_result.get('frames_analyzed', 0)
                            descriptions = []
                            
                            if 'results' in analysis_result:
                                for result in analysis_result['results'][:3]:  # First 3 analyses
                                    if 'analysis' in result:
                                        descriptions.append(str(result['analysis']))
                            
                            response = f"I'm now continuously monitoring your screen at 30 FPS. I've analyzed {frames_analyzed} frames in the last 5 seconds.\n\n"
                            
                            if descriptions:
                                response += "Here's what I observed:\n" + "\n".join(f"• {desc[:100]}..." for desc in descriptions)
                            else:
                                response += "I'm watching your screen for any changes or important events."
                                
                            return response
                        else:
                            return "I've started monitoring your screen. I'll watch for changes and alert you to anything important."
                            
            except Exception as e:
                logger.error(f"Error in monitoring command: {e}")
                return f"I encountered an error setting up continuous monitoring: {str(e)}. Let me fall back to standard screenshot analysis."
                
        else:
            logger.warning("[MONITORING] Vision analyzer is None - cannot start monitoring")
        
        # Fallback response if enhanced analyzer not available
        return "I'll need the enhanced vision system to enable continuous monitoring. Currently, I can only take screenshots on demand. Please ensure the vision system is properly initialized."
    
    def _is_screen_query(self, user_input: str) -> bool:
        """Check if user is asking about what's currently on screen"""
        screen_query_patterns = [
            r'can you see',
            r'do you see',
            r'what do you see',
            r'what.*on.*screen',
            r'what.*looking at',
            r'describe.*screen',
            r'tell me what',
            r'what is on',
            r'what\'s on',
            r'what are you seeing',
            r'what can you see'
        ]
        
        input_lower = user_input.lower()
        for pattern in screen_query_patterns:
            if re.search(pattern, input_lower):
                return True
        return False
    
    async def _analyze_current_screen(self, query: str) -> str:
        """Analyze current screen content when monitoring is active"""
        try:
            logger.info(f"[MONITOR] Real-time screen analysis requested: {query}")
            
            # Capture current screen
            try:
                screenshot = await self.capture_screenshot()
            except Exception as e:
                logger.error(f"[MONITOR] Screenshot capture failed: {e}")
                return f"I encountered an error capturing your screen: {str(e)}. Please ensure screen recording permissions are enabled."
                
            if not screenshot:
                return "I'm having trouble capturing the screen right now. Please ensure screen recording permissions are enabled in System Preferences."
            
            # Create an enhanced prompt with pure vision intelligence
            query_lower = query.lower()
            
            # Base intelligence framework for real-time monitoring
            monitoring_intelligence = (
                "You are JARVIS, Tony Stark's AI assistant, actively monitoring the screen in real-time.\n"
                "Use your advanced vision intelligence to:\n"
                "- UNDERSTAND the complete visual context including window relationships\n"
                "- DISTINGUISH between system UI and application content\n"
                "- RECOGNIZE when elements are obscured, minimized, or on different screens\n"
                "- EXPRESS CONFIDENCE levels when there's ambiguity\n"
                "- PROVIDE INTELLIGENT INSIGHTS about what's happening\n\n"
            )
            
            if 'battery' in query_lower or 'power' in query_lower or 'charge' in query_lower:
                analysis_prompt = (f"{monitoring_intelligence}"
                    f"The user asked: '{query}'\n\n"
                    "REAL-TIME BATTERY ANALYSIS:\n"
                    "- Locate the SYSTEM battery indicator (not battery icons in apps/screenshots)\n"
                    "- Report the exact percentage visible\n"
                    "- Note charging state\n"
                    "- If multiple batteries visible, identify the system one\n"
                    "- If obscured or not visible, explain why")
            
            elif 'time' in query_lower or 'clock' in query_lower:
                analysis_prompt = (f"{monitoring_intelligence}"
                    f"The user asked: '{query}'\n\n"
                    "REAL-TIME TIME CHECK:\n"
                    "- Find the SYSTEM clock (not times in apps/videos)\n"
                    "- Report the exact time displayed\n"
                    "- If multiple times visible, explain which is the system time\n"
                    "- Express confidence if ambiguous")
            
            elif 'status bar' in query_lower or 'menu bar' in query_lower:
                analysis_prompt = (f"{monitoring_intelligence}"
                    f"The user asked: '{query}'\n\n"
                    "REAL-TIME STATUS BAR ANALYSIS:\n"
                    "- Examine the entire system status/menu bar\n"
                    "- Report all elements with exact values\n"
                    "- Note any parts hidden by windows\n"
                    "- Distinguish from app toolbars")
            
            elif any(app in query_lower for app in ['slack', 'discord', 'chrome', 'safari', 'vscode', 'terminal']):
                # Extract the app name
                app_name = next((app for app in ['slack', 'discord', 'chrome', 'safari', 'vscode', 'terminal'] if app in query_lower), None)
                analysis_prompt = (f"{monitoring_intelligence}"
                    f"The user asked: '{query}'\n\n"
                    f"REAL-TIME {app_name.upper()} CHECK:\n"
                    f"- Is {app_name.title()} visible on screen?\n"
                    f"- If yes: Is it in foreground or background? What's visible?\n"
                    f"- If no: Explain (minimized, different desktop, not running)\n"
                    f"- Don't assume - only report what you actually see")
            
            else:
                # Comprehensive screen analysis
                analysis_prompt = (f"{monitoring_intelligence}"
                    f"The user asked: '{query}'\n\n"
                    "REAL-TIME COMPREHENSIVE ANALYSIS:\n"
                    "- Identify ALL visible windows and their z-order\n"
                    "- Note which window is active/focused\n"
                    "- Report exact system UI values\n"
                    "- Describe window relationships and occlusions\n"
                    "- Mention any dynamic content or transitions\n"
                    "- If they ask about something not visible, explain why\n"
                    "- Provide intelligent insights about the user's workflow")
            
            # Add temporal awareness for monitoring
            analysis_prompt += (
                "\n\nREAL-TIME CONTEXT: Since you're monitoring in real-time, "
                "note if anything appears to be changing or if the user might need "
                "to know about something that just happened or is about to happen."
            )
            
            # Analyze the screenshot
            messages = [
                {
                    "role": "user",
                    "content": [
                        {
                            "type": "text",
                            "text": analysis_prompt
                        },
                        {
                            "type": "image",
                            "source": {
                                "type": "base64",
                                "media_type": "image/png",
                                "data": self._encode_image_optimized(screenshot)
                            }
                        }
                    ]
                }
            ]
            
            # Make API call with vision
            try:
                response = await asyncio.to_thread(
                    self.client.messages.create,
                    model=self.model,
                    max_tokens=300,
                    temperature=0.7,
                    messages=messages,
                    system="You are JARVIS, an AI assistant with real-time screen monitoring capabilities. Provide natural, conversational responses about what you observe on the user's screen."
                )
                
                if response and hasattr(response, 'content') and len(response.content) > 0:
                    return response.content[0].text
                else:
                    logger.error("[MONITOR] Empty response from Claude API")
                    return "I'm having trouble analyzing the screen right now. The vision API returned an empty response."
                    
            except Exception as api_error:
                logger.error(f"[MONITOR] Claude API error: {api_error}")
                if "api_key" in str(api_error).lower():
                    return "I need my vision API key to analyze your screen. Please ensure ANTHROPIC_API_KEY is set."
                elif "rate_limit" in str(api_error).lower():
                    return "I'm being rate limited by the vision API. Please try again in a moment."
                else:
                    return f"I encountered an error with the vision API: {str(api_error)}. Please try again."
            
        except Exception as e:
            logger.error(f"[MONITOR] Error analyzing current screen: {e}", exc_info=True)
            return "I'm having trouble analyzing the screen right now. Let me try taking a fresh screenshot."
        
    async def generate_response(self, user_input: str) -> str:
        """
        Process user input and generate response, using vision when appropriate
        """
        if not self.is_available():
            return "Claude API is not available. Please install anthropic package and set API key."
            
        # Check for continuous monitoring commands
        is_monitoring = await self._is_monitoring_command(user_input)
        logger.info(f"[VISION DEBUG] Is monitoring command: {is_monitoring} for input: {user_input}")
        
        if is_monitoring:
            logger.info(f"[VISION DEBUG] Routing to _handle_monitoring_command")
            return await self._handle_monitoring_command(user_input)
            
        # Check if monitoring is active and user is asking about what's on screen
        if self._monitoring_active and self._is_screen_query(user_input):
            logger.info(f"[VISION DEBUG] Monitoring active and screen query detected: {user_input}")
            # Use real-time screen analysis when monitoring is active
            return await self._analyze_current_screen(user_input)
            
        # Check if this is a vision command
        if self.is_vision_command(user_input):
            logger.info(f"Vision command detected: {user_input}")
            return await self.analyze_screen_with_vision(user_input)
            
        # Otherwise, use regular text processing
        try:
            # Build messages for the API
            messages = self._build_messages(user_input)
            
            # Update system prompt with current date
            # Use correct year - datetime.now() returns actual current date
            current_datetime = datetime.now().strftime("%A, %B %d, %Y at %I:%M %p")
            dynamic_system_prompt = f"""{self.system_prompt}

Note: The current date and time is {current_datetime}. Always use this as the reference for any time-related queries."""
            
            # Make API call
            start_time = datetime.now()
            
            response = await asyncio.to_thread(
                self.client.messages.create,
                model=self.model,
                max_tokens=self.max_tokens,
                temperature=self.temperature,
                system=dynamic_system_prompt,
                messages=messages
            )
            
            # Extract response
            ai_response = response.content[0].text
            
            # Log performance
            response_time = (datetime.now() - start_time).total_seconds()
            logger.info(f"Claude API call completed in {response_time:.2f}s")
            
            # Update conversation history
            self._update_history(user_input, ai_response)
            
            return ai_response
            
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            return f"I encountered an error: {str(e)}"
            
    def _build_messages(self, user_input: str, context_window: Optional[int] = None, 
                       include_system_messages: bool = False,
                       filter_strategy: Optional[str] = None) -> List[Dict[str, Any]]:
        """Build message list with intelligent context management"""
        messages = []
        
        # Determine context window size dynamically
        if context_window is None:
            context_window = self._calculate_optimal_context_window(user_input)
            
        # Get relevant history based on strategy
        relevant_history = self._get_relevant_history(context_window, filter_strategy)
        
        # Add system messages if needed
        if include_system_messages and hasattr(self, '_last_system_message'):
            messages.append({"role": "system", "content": self._last_system_message})
            
        # Build conversation context intelligently
        for entry in relevant_history:
            # Add user message
            user_msg = {
                "role": "user", 
                "content": entry["user"]
            }
            
            # Add metadata if available
            if "user_metadata" in entry:
                user_msg["metadata"] = entry["user_metadata"]
                
            messages.append(user_msg)
            
            # Add assistant message
            assistant_msg = {
                "role": "assistant", 
                "content": entry["assistant"]
            }
            
            if "assistant_metadata" in entry:
                assistant_msg["metadata"] = entry["assistant_metadata"]
                
            messages.append(assistant_msg)
            
        # Add current user input with enhanced context
        current_msg = {
            "role": "user", 
            "content": user_input
        }
        
        # Add query metadata
        if hasattr(self, '_last_vision_intent'):
            current_msg["metadata"] = {
                "intent": self._last_vision_intent,
                "timestamp": datetime.now().isoformat(),
                "is_vision": self.is_vision_command(user_input)
            }
            
        messages.append(current_msg)
        
        # Validate message size
        messages = self._validate_message_size(messages)
        
        return messages
    
    def _calculate_optimal_context_window(self, user_input: str) -> int:
        """Calculate optimal context window based on query type"""
        # Base window size
        base_window = 5
        
        # Adjust based on query complexity
        if self.is_vision_command(user_input):
            # Vision commands may need less context
            base_window = 3
        elif any(keyword in user_input.lower() for keyword in ['continue', 'more', 'explain', 'why']):
            # Follow-up questions need more context
            base_window = 7
        elif len(user_input) > 200:
            # Complex queries benefit from more context
            base_window = 6
            
        # Adjust based on available history
        available_history = len(self.conversation_history)
        
        # Dynamic adjustment based on conversation flow
        if available_history > 0:
            recent_lengths = [len(h['user']) + len(h['assistant']) 
                            for h in self.conversation_history[-3:]]
            avg_length = sum(recent_lengths) / len(recent_lengths) if recent_lengths else 0
            
            # Reduce context for very long conversations
            if avg_length > 1000:
                base_window = max(2, base_window - 2)
            
        return min(base_window, available_history)
    
    def _get_relevant_history(self, context_window: int, filter_strategy: Optional[str]) -> List[Dict]:
        """Get relevant history based on filtering strategy"""
        if not self.conversation_history:
            return []
            
        if filter_strategy == 'importance':
            # Score conversations by importance
            scored_history = []
            for i, entry in enumerate(self.conversation_history):
                score = self._calculate_conversation_importance(entry, i)
                scored_history.append((score, entry))
                
            # Sort by importance and take top entries
            scored_history.sort(key=lambda x: x[0], reverse=True)
            return [entry for _, entry in scored_history[:context_window]]
            
        elif filter_strategy == 'semantic':
            # Get semantically similar conversations
            return self._get_semantically_similar_history(context_window)
            
        elif filter_strategy == 'recent_topics':
            # Filter by recent topics
            recent_topics = self._extract_recent_topics()
            filtered = []
            for entry in reversed(self.conversation_history):
                if any(topic in entry['user'].lower() for topic in recent_topics):
                    filtered.append(entry)
                    if len(filtered) >= context_window:
                        break
            return list(reversed(filtered))
            
        else:
            # Default: most recent conversations
            return self.conversation_history[-context_window:]
    
    def _calculate_conversation_importance(self, entry: Dict, index: int) -> float:
        """Calculate importance score for a conversation"""
        score = 0.0
        
        # Recency factor (more recent = higher score)
        recency_weight = (index + 1) / len(self.conversation_history)
        score += recency_weight * 0.3
        
        # Length factor (longer = more substantial)
        total_length = len(entry['user']) + len(entry['assistant'])
        length_score = min(total_length / 500, 1.0)
        score += length_score * 0.2
        
        # Keyword importance
        important_keywords = ['error', 'important', 'remember', 'note', 'key', 'critical']
        keyword_count = sum(1 for kw in important_keywords if kw in entry['user'].lower() or kw in entry['assistant'].lower())
        score += (keyword_count * 0.1)
        
        # Vision relevance
        if self.is_vision_command(entry['user']):
            score += 0.2
            
        # Question/Answer quality
        if '?' in entry['user'] and len(entry['assistant']) > 100:
            score += 0.2
            
        return score
    
    def _validate_message_size(self, messages: List[Dict]) -> List[Dict]:
        """Validate and trim messages to fit API limits"""
        # Estimate token count (rough approximation)
        total_chars = sum(len(msg.get('content', '')) for msg in messages)
        estimated_tokens = total_chars / 4  # Rough estimate
        
        # Claude's context limit (adjust based on model)
        max_tokens = 100000 if 'claude-3' in self.model else 9000
        
        # If within limits, return as-is
        if estimated_tokens < max_tokens * 0.8:  # Leave 20% buffer
            return messages
            
        # Intelligent trimming
        logger.warning(f"Message size ({estimated_tokens} tokens) exceeds limit, trimming...")
        
        # Keep system messages and current query
        essential_messages = [
            msg for msg in messages 
            if msg.get('role') == 'system' or msg == messages[-1]
        ]
        
        # Progressively remove older messages
        other_messages = [msg for msg in messages if msg not in essential_messages]
        
        while len(other_messages) > 2 and estimated_tokens > max_tokens * 0.8:
            # Remove oldest exchange (user + assistant)
            if len(other_messages) >= 2:
                other_messages = other_messages[2:]
                
            # Recalculate
            total_chars = sum(len(msg.get('content', '')) for msg in essential_messages + other_messages)
            estimated_tokens = total_chars / 4
            
        return essential_messages + other_messages
    
    def _extract_recent_topics(self) -> List[str]:
        """Extract recent topics from conversation"""
        topics = []
        
        # Simple topic extraction from recent conversations
        for entry in self.conversation_history[-3:]:
            text = entry['user'].lower() + ' ' + entry['assistant'].lower()
            
            # Extract nouns and key phrases
            topic_indicators = ['about', 'regarding', 'concerning', 'related to']
            for indicator in topic_indicators:
                if indicator in text:
                    # Extract words after indicator
                    parts = text.split(indicator)
                    if len(parts) > 1:
                        potential_topic = parts[1].split()[0:3]
                        topics.extend(potential_topic)
                        
        return list(set(topics))  # Unique topics
        
    def _update_history(self, user_input: str, ai_response: str, 
                       metadata: Optional[Dict[str, Any]] = None,
                       preserve_important: bool = True):
        """Update conversation history with intelligent management"""
        # Create enhanced history entry
        history_entry = {
            "user": user_input,
            "assistant": ai_response,
            "timestamp": datetime.now().isoformat(),
            "id": self._generate_conversation_id(),
            "metadata": {
                "user_length": len(user_input),
                "assistant_length": len(ai_response),
                "is_vision": self.is_vision_command(user_input),
                "model_used": self.model,
                "temperature": self.temperature,
            }
        }
        
        # Add additional metadata if provided
        if metadata:
            history_entry["metadata"].update(metadata)
            
        # Add performance metrics if available
        if hasattr(self, '_last_response_time'):
            history_entry["metadata"]["response_time_ms"] = self._last_response_time
            
        # Add to history
        self.conversation_history.append(history_entry)
        
        # Update analytics
        if hasattr(self, '_usage_analytics'):
            if history_entry["metadata"].get("is_vision"):
                self._usage_analytics["vision_requests"] += 1
                
        # Intelligent history management
        if len(self.conversation_history) > self.max_history_length:
            if preserve_important:
                self._intelligent_history_trim()
            else:
                # Simple FIFO trimming
                self.conversation_history = self.conversation_history[-self.max_history_length:]
                
        # Trigger cleanup if needed
        if len(self.conversation_history) > self.max_history_length * 1.5:
            asyncio.create_task(self.optimize_for_performance())
    
    def _generate_conversation_id(self) -> str:
        """Generate unique conversation ID"""
        timestamp = datetime.now().timestamp()
        random_component = hashlib.md5(str(timestamp).encode()).hexdigest()[:8]
        return f"conv_{int(timestamp)}_{random_component}"
    
    def _intelligent_history_trim(self):
        """Trim history while preserving important conversations"""
        if len(self.conversation_history) <= self.max_history_length:
            return
            
        # Score all conversations
        scored_conversations = []
        for i, entry in enumerate(self.conversation_history):
            score = self._calculate_preservation_score(entry, i)
            scored_conversations.append((score, i, entry))
            
        # Sort by score (higher = more important)
        scored_conversations.sort(key=lambda x: x[0], reverse=True)
        
        # Keep top conversations up to max_history_length
        conversations_to_keep = []
        for score, original_index, entry in scored_conversations[:self.max_history_length]:
            conversations_to_keep.append((original_index, entry))
            
        # Sort by original index to maintain order
        conversations_to_keep.sort(key=lambda x: x[0])
        
        # Update history
        self.conversation_history = [entry for _, entry in conversations_to_keep]
        
        logger.info(f"Intelligently trimmed history from {len(scored_conversations)} to {len(self.conversation_history)} entries")
    
    def _calculate_preservation_score(self, entry: Dict, index: int) -> float:
        """Calculate score for preserving a conversation"""
        score = 0.0
        metadata = entry.get('metadata', {})
        
        # Recency (exponential decay)
        age_factor = index / len(self.conversation_history)
        score += (1 - age_factor) * 0.3
        
        # Vision queries are often important
        if metadata.get('is_vision'):
            score += 0.25
            
        # Long responses indicate substantial content
        if metadata.get('assistant_length', 0) > 500:
            score += 0.2
            
        # Error or important keywords
        important_patterns = ['error', 'important', 'remember', 'save', 'critical', 'bug', 'issue']
        text = (entry.get('user', '') + entry.get('assistant', '')).lower()
        if any(pattern in text for pattern in important_patterns):
            score += 0.3
            
        # Questions with detailed answers
        if '?' in entry.get('user', '') and metadata.get('assistant_length', 0) > 200:
            score += 0.15
            
        # Performance anomalies (very slow or fast responses)
        response_time = metadata.get('response_time_ms', 0)
        if response_time > 5000 or (response_time > 0 and response_time < 100):
            score += 0.1  # Might be interesting edge cases
            
        return score
            
    async def generate_response_with_context(self, 
                                           user_input: str,
                                           include_analytics: bool = True,
                                           include_suggestions: bool = True,
                                           custom_context: Optional[Dict] = None) -> Dict[str, Any]:
        """Generate response with comprehensive context and metadata"""
        # Track timing
        start_time = datetime.now()
        
        # Analyze input before processing
        input_analysis = self._analyze_user_input(user_input)
        
        # Generate response with error handling
        try:
            response = await self.generate_response(user_input)
            success = True
            error_info = None
        except Exception as e:
            logger.error(f"Error generating response: {e}")
            response = self._get_error_response(e)
            success = False
            error_info = str(e)
            
        # Calculate metrics
        response_time = (datetime.now() - start_time).total_seconds() * 1000
        
        # Build comprehensive context
        context_data = {
            "response": response,
            "success": success,
            "conversation_id": self._get_or_create_conversation_id(),
            "message_id": self._generate_message_id(),
            "timestamp": datetime.now().isoformat(),
            "response_time_ms": response_time,
            
            # Conversation state
            "conversation": {
                "message_count": len(self.conversation_history),
                "session_duration": self._calculate_session_duration(),
                "topics": self._extract_conversation_topics(),
                "context_window_used": self._get_last_context_window_size()
            },
            
            # Model information
            "model_info": {
                "model": self.model,
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "vision_capable": True,
                "model_type": self._get_model_type()
            },
            
            # Input analysis
            "input_analysis": input_analysis,
            
            # Platform capabilities
            "capabilities": {
                "vision_available": self.is_available(),
                "screenshot_methods": len(getattr(self, '_capture_methods', [])),
                "platform": getattr(self, '_platform', 'unknown')
            }
        }
        
        # Add error info if applicable
        if error_info:
            context_data["error"] = {
                "message": error_info,
                "type": type(e).__name__ if 'e' in locals() else "Unknown",
                "timestamp": datetime.now().isoformat()
            }
            
        # Add analytics if requested
        if include_analytics and hasattr(self, '_usage_analytics'):
            context_data["analytics"] = self._get_session_analytics()
            
        # Add suggestions if requested
        if include_suggestions:
            context_data["suggestions"] = self._generate_follow_up_suggestions(user_input, response)
            
        # Add custom context if provided
        if custom_context:
            context_data["custom"] = custom_context
            
        # Update internal tracking
        self._last_response_time = response_time
        self._last_context_data = context_data
        
        return context_data
    
    def _analyze_user_input(self, user_input: str) -> Dict[str, Any]:
        """Analyze user input for insights"""
        analysis = {
            "length": len(user_input),
            "word_count": len(user_input.split()),
            "is_question": '?' in user_input,
            "is_command": any(cmd in user_input.lower() for cmd in ['show', 'tell', 'explain', 'describe']),
            "is_vision": self.is_vision_command(user_input),
            "sentiment": self._detect_simple_sentiment(user_input),
            "complexity": self._estimate_query_complexity(user_input),
            "language_hints": self._detect_language_hints(user_input)
        }
        
        # Add intent if vision command
        if analysis["is_vision"] and hasattr(self, '_last_vision_intent'):
            analysis["vision_intent"] = self._last_vision_intent
            
        return analysis
    
    def _detect_simple_sentiment(self, text: str) -> str:
        """Detect basic sentiment from text"""
        positive_words = ['thanks', 'great', 'awesome', 'perfect', 'excellent', 'good']
        negative_words = ['error', 'wrong', 'bad', 'issue', 'problem', 'fail']
        
        text_lower = text.lower()
        positive_count = sum(1 for word in positive_words if word in text_lower)
        negative_count = sum(1 for word in negative_words if word in text_lower)
        
        if positive_count > negative_count:
            return "positive"
        elif negative_count > positive_count:
            return "negative"
        else:
            return "neutral"
    
    def _estimate_query_complexity(self, text: str) -> str:
        """Estimate query complexity"""
        word_count = len(text.split())
        
        # Check for complex indicators
        complex_indicators = ['how', 'why', 'explain', 'analyze', 'compare', 'multiple']
        has_complex = any(indicator in text.lower() for indicator in complex_indicators)
        
        if word_count > 50 or has_complex:
            return "complex"
        elif word_count > 20:
            return "moderate"
        else:
            return "simple"
    
    def _detect_language_hints(self, text: str) -> List[str]:
        """Detect language or communication hints"""
        hints = []
        
        text_lower = text.lower()
        
        # Formality
        if any(word in text_lower for word in ['please', 'could you', 'would you']):
            hints.append("polite")
            
        # Urgency
        if any(word in text_lower for word in ['urgent', 'asap', 'quickly', 'now']):
            hints.append("urgent")
            
        # Technical
        if any(word in text_lower for word in ['api', 'code', 'debug', 'error', 'function']):
            hints.append("technical")
            
        return hints
    
    def _get_or_create_conversation_id(self) -> str:
        """Get or create conversation ID for session"""
        if not hasattr(self, '_conversation_id'):
            self._conversation_id = f"conv_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{hashlib.md5(str(id(self)).encode()).hexdigest()[:8]}"
        return self._conversation_id
    
    def _generate_message_id(self) -> str:
        """Generate unique message ID"""
        return f"msg_{int(datetime.now().timestamp() * 1000)}_{hashlib.md5(str(datetime.now()).encode()).hexdigest()[:6]}"
    
    def _calculate_session_duration(self) -> float:
        """Calculate current session duration in seconds"""
        if not self.conversation_history:
            return 0.0
            
        first_timestamp = datetime.fromisoformat(self.conversation_history[0]['timestamp'])
        return (datetime.now() - first_timestamp).total_seconds()
    
    def _extract_conversation_topics(self) -> List[str]:
        """Extract main topics from conversation"""
        topics = set()
        
        # Analyze recent conversations
        for entry in self.conversation_history[-5:]:
            text = entry['user'] + ' ' + entry['assistant']
            
            # Simple topic extraction
            if 'vision' in text.lower() or 'screen' in text.lower():
                topics.add('vision_analysis')
            if 'error' in text.lower() or 'debug' in text.lower():
                topics.add('troubleshooting')
            if 'code' in text.lower() or 'program' in text.lower():
                topics.add('programming')
            if 'help' in text.lower() or 'how' in text.lower():
                topics.add('assistance')
                
        return list(topics)
    
    def _get_last_context_window_size(self) -> int:
        """Get the size of the last context window used"""
        return getattr(self, '_last_context_window_size', 0)
    
    def _get_session_analytics(self) -> Dict[str, Any]:
        """Get current session analytics"""
        analytics = {
            "total_messages": len(self.conversation_history),
            "vision_queries": sum(1 for h in self.conversation_history if h.get('metadata', {}).get('is_vision')),
            "avg_response_time_ms": self._usage_analytics.get('avg_response_time', 0),
            "error_count": self._usage_analytics.get('errors', 0),
            "cache_hit_rate": self._calculate_cache_hit_rate() if hasattr(self, '_calculate_cache_hit_rate') else 0
        }
        
        return analytics
    
    def _generate_follow_up_suggestions(self, user_input: str, response: str) -> List[str]:
        """Generate intelligent follow-up suggestions"""
        suggestions = []
        
        # Based on query type
        if self.is_vision_command(user_input):
            suggestions.extend([
                "Can you analyze a specific part of the screen?",
                "What else do you notice in the image?",
                "Can you describe the layout in more detail?"
            ])
        elif '?' in user_input:
            suggestions.extend([
                "Would you like more details?",
                "Can I clarify anything else?",
                "Do you have a follow-up question?"
            ])
            
        # Based on response content
        if 'error' in response.lower():
            suggestions.append("How can I help troubleshoot this issue?")
        elif len(response) > 500:
            suggestions.append("Would you like a summary of the key points?")
            
        return suggestions[:3]  # Limit to 3 suggestions
    
    def _get_error_response(self, error: Exception) -> str:
        """Generate appropriate error response"""
        error_type = type(error).__name__
        
        if "API" in error_type:
            return "I'm experiencing API connectivity issues. Please try again in a moment."
        elif "Permission" in error_type:
            return "I need additional permissions to complete this request. Please check system settings."
        elif "Timeout" in error_type:
            return "The request took too long to process. Please try again with a simpler query."
        else:
            return f"I encountered an unexpected error: {error_type}. Please try again."
        
    async def clear_history(self, preserve_context: bool = False, preserve_last_n: int = 0):
        """Enhanced history clearing with intelligent options"""
        if not hasattr(self, '_history_metadata'):
            self._history_metadata = {}
            
        # Store metadata about the cleared history
        cleared_count = len(self.conversation_history)
        cleared_timestamp = datetime.now()
        
        if preserve_context:
            # Preserve important context from history
            context_summary = await self._extract_context_summary()
            self._history_metadata['last_context'] = context_summary
            
        if preserve_last_n > 0:
            # Keep the most recent N exchanges
            preserved = self.conversation_history[-preserve_last_n:]
            self.conversation_history.clear()
            self.conversation_history.extend(preserved)
            logger.info(f"Conversation history cleared, preserved last {preserve_last_n} exchanges")
        else:
            # Complete clear
            self.conversation_history.clear()
            logger.info(f"Conversation history cleared ({cleared_count} exchanges removed)")
            
        # Track clearing events for analytics
        if not hasattr(self, '_clear_history_log'):
            self._clear_history_log = []
            
        self._clear_history_log.append({
            'timestamp': cleared_timestamp,
            'cleared_count': cleared_count,
            'preserved_count': preserve_last_n,
            'preserve_context': preserve_context
        })
        
        # Cleanup old logs (keep last 100)
        if len(self._clear_history_log) > 100:
            self._clear_history_log = self._clear_history_log[-100:]
        
    def is_available(self) -> bool:
        """Comprehensive availability check with diagnostics"""
        # Basic availability
        basic_available = ANTHROPIC_AVAILABLE and self.client is not None
        
        if not basic_available:
            return False
            
        # Extended health checks
        if not hasattr(self, '_last_health_check'):
            self._last_health_check = None
            self._health_check_cache = None
            
        # Cache health check for 60 seconds
        if (self._last_health_check is None or 
            datetime.now() - self._last_health_check > timedelta(seconds=60)):
            
            self._health_check_cache = self._perform_health_check()
            self._last_health_check = datetime.now()
            
        return self._health_check_cache
    
    def _perform_health_check(self) -> bool:
        """Perform comprehensive health check"""
        checks = {
            'api_key': bool(self.api_key),
            'client': self.client is not None,
            'model_valid': self._is_valid_model(self.model),
            'screenshot_available': self._check_screenshot_capability(),
            'vision_analyzer': self.vision_analyzer is not None
        }
        
        # Log any issues
        failed_checks = [k for k, v in checks.items() if not v]
        if failed_checks:
            logger.warning(f"Health check failures: {failed_checks}")
            
        # Require at least API key and client
        return checks['api_key'] and checks['client']
    
    def _is_valid_model(self, model: str) -> bool:
        """Check if model is valid and supported"""
        valid_models = [
            'claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku',
            'claude-3-5-sonnet', 'claude-2.1', 'claude-2.0',
            'claude-instant-1.2'
        ]
        return any(valid in model.lower() for valid in valid_models)
    
    def _check_screenshot_capability(self) -> bool:
        """Check screenshot capability for current platform"""
        if SCREENSHOT_AVAILABLE:
            return True
            
        # Check platform-specific alternatives
        if hasattr(self, '_capture_methods'):
            return len(self._capture_methods) > 0
            
        return False
        
    def get_usage_stats(self) -> Dict[str, Any]:
        """Get comprehensive usage statistics and analytics"""
        # Initialize analytics tracking if needed
        if not hasattr(self, '_usage_analytics'):
            self._usage_analytics = {
                'total_requests': 0,
                'vision_requests': 0,
                'errors': 0,
                'total_tokens': 0,
                'cache_hits': 0,
                'cache_misses': 0,
                'avg_response_time': 0,
                'response_times': []
            }
            
        # Calculate dynamic statistics
        stats = {
            # Basic info
            "model": self.model,
            "model_type": self._get_model_type(),
            "api_available": self.is_available(),
            
            # History stats
            "history": {
                "current_length": len(self.conversation_history),
                "max_length": self.max_history_length,
                "total_characters": sum(len(h.get('user', '')) + len(h.get('assistant', '')) 
                                       for h in self.conversation_history),
                "avg_exchange_length": self._calculate_avg_exchange_length()
            },
            
            # Vision capabilities
            "vision": {
                "capable": True,
                "screenshot_available": SCREENSHOT_AVAILABLE,
                "capture_methods": len(getattr(self, '_capture_methods', [])),
                "analyzer_available": self.vision_analyzer is not None,
                "supported_formats": self._get_supported_image_formats()
            },
            
            # Performance metrics
            "performance": {
                "total_requests": self._usage_analytics['total_requests'],
                "vision_requests": self._usage_analytics['vision_requests'],
                "error_rate": self._calculate_error_rate(),
                "avg_response_time_ms": self._usage_analytics['avg_response_time'],
                "cache_hit_rate": self._calculate_cache_hit_rate()
            },
            
            # Platform info
            "platform": {
                "os": platform.system(),
                "python_version": platform.python_version(),
                "timezone": self._get_timezone_name() or "Unknown"
            },
            
            # Configuration
            "config": {
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "cache_duration": getattr(self, '_cache_config', {}).get('duration', timedelta(seconds=5)).total_seconds()
            }
        }
        
        return stats
    
    def _get_model_type(self) -> str:
        """Determine model type from model name"""
        model_lower = self.model.lower()
        if 'opus' in model_lower:
            return 'opus'
        elif 'sonnet' in model_lower:
            return 'sonnet'
        elif 'haiku' in model_lower:
            return 'haiku'
        elif 'instant' in model_lower:
            return 'instant'
        else:
            return 'unknown'
    
    def _calculate_avg_exchange_length(self) -> float:
        """Calculate average conversation exchange length"""
        if not self.conversation_history:
            return 0.0
            
        total_length = sum(len(h.get('user', '')) + len(h.get('assistant', '')) 
                          for h in self.conversation_history)
        return total_length / len(self.conversation_history)
    
    def _get_supported_image_formats(self) -> List[str]:
        """Get list of supported image formats"""
        formats = ['JPEG', 'PNG']
        
        try:
            from PIL import Image
            # Add more formats if PIL supports them
            additional_formats = ['GIF', 'BMP', 'WEBP']
            for fmt in additional_formats:
                try:
                    Image.new('RGB', (1, 1)).save(io.BytesIO(), format=fmt)
                    formats.append(fmt)
                except:
                    pass
        except:
            pass
            
        return formats
    
    def _calculate_error_rate(self) -> float:
        """Calculate error rate percentage"""
        total = self._usage_analytics['total_requests']
        if total == 0:
            return 0.0
        return (self._usage_analytics['errors'] / total) * 100
    
    def _calculate_cache_hit_rate(self) -> float:
        """Calculate cache hit rate percentage"""
        total_cache_requests = (self._usage_analytics['cache_hits'] + 
                               self._usage_analytics['cache_misses'])
        if total_cache_requests == 0:
            return 0.0
        return (self._usage_analytics['cache_hits'] / total_cache_requests) * 100
        
    @property
    def model_name(self) -> str:
        """Get model name with dynamic information"""
        # Add context about model capabilities
        base_model = self.model
        
        # Add capability indicators
        capabilities = []
        if 'vision' in base_model.lower() or '3' in base_model:
            capabilities.append('vision')
        if 'instant' in base_model.lower() or 'haiku' in base_model.lower():
            capabilities.append('fast')
        if 'opus' in base_model.lower():
            capabilities.append('advanced')
            
        if capabilities:
            return f"{base_model} [{', '.join(capabilities)}]"
        return base_model
        
    async def generate_response_stream(self, user_input: str, adaptive: bool = True):
        """Enhanced streaming with adaptive chunk sizes and timing"""
        # Check if this is a vision command
        is_vision = self.is_vision_command(user_input)
        
        # Get the response
        start_time = datetime.now()
        response = await self.generate_response(user_input)
        response_time = (datetime.now() - start_time).total_seconds()
        
        # Determine optimal streaming parameters
        if adaptive:
            stream_params = self._calculate_stream_parameters(response, response_time, is_vision)
        else:
            stream_params = {'chunk_size': 50, 'delay': 0.01}
            
        # Track streaming metrics
        chunks_sent = 0
        total_delay = 0
        
        # Stream with dynamic parameters
        chunk_size = stream_params['chunk_size']
        base_delay = stream_params['delay']
        
        for i in range(0, len(response), chunk_size):
            chunk = response[i:i+chunk_size]
            
            # Dynamic delay based on content
            delay = self._calculate_chunk_delay(chunk, base_delay, chunks_sent)
            
            yield chunk
            
            await asyncio.sleep(delay)
            chunks_sent += 1
            total_delay += delay
            
            # Adaptive adjustment mid-stream
            if adaptive and chunks_sent % 10 == 0:
                # Adjust parameters based on performance
                remaining = len(response) - i
                if remaining > chunk_size * 5:  # Still have significant content
                    chunk_size = min(chunk_size * 2, 200)  # Speed up
                    
        # Log streaming metrics
        if hasattr(self, '_streaming_metrics'):
            self._streaming_metrics.append({
                'response_length': len(response),
                'chunks': chunks_sent,
                'total_delay': total_delay,
                'avg_chunk_size': len(response) / chunks_sent if chunks_sent > 0 else 0,
                'is_vision': is_vision
            })
    
    def _calculate_stream_parameters(self, response: str, response_time: float, is_vision: bool) -> Dict[str, Any]:
        """Calculate optimal streaming parameters"""
        response_length = len(response)
        
        # Base parameters
        params = {
            'chunk_size': 50,
            'delay': 0.01
        }
        
        # Adjust for response length
        if response_length < 200:
            # Short response - larger chunks
            params['chunk_size'] = 100
            params['delay'] = 0.02
        elif response_length > 2000:
            # Long response - smaller initial chunks, will adapt
            params['chunk_size'] = 30
            params['delay'] = 0.005
            
        # Adjust for response time (simulate realistic typing)
        if response_time < 1.0:
            # Fast response - slow down streaming to seem more natural
            params['delay'] *= 2
        elif response_time > 5.0:
            # Slow response - speed up streaming
            params['delay'] *= 0.5
            
        # Adjust for content type
        if is_vision:
            # Vision responses often have structured content
            params['chunk_size'] = 75  # Larger chunks for descriptions
            
        return params
    
    def _calculate_chunk_delay(self, chunk: str, base_delay: float, chunk_index: int) -> float:
        """Calculate delay for specific chunk based on content"""
        # Natural pauses at punctuation
        if chunk.rstrip().endswith(('.', '!', '?')):
            return base_delay * 3  # Longer pause at sentence end
        elif chunk.rstrip().endswith((',', ';', ':')):
            return base_delay * 2  # Medium pause at clause breaks
        elif '\n' in chunk:
            return base_delay * 2.5  # Pause at line breaks
            
        # Speed up after initial chunks
        if chunk_index > 5:
            return base_delay * 0.8
            
        return base_delay
            
    async def get_conversation_history(self, 
                                       include_metadata: bool = False,
                                       last_n: Optional[int] = None,
                                       filter_by: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get conversation history with filtering and metadata options"""
        history = self.conversation_history.copy()
        
        # Apply filtering
        if filter_by:
            if filter_by == 'vision':
                # Filter for vision-related exchanges
                history = [h for h in history 
                          if self.is_vision_command(h.get('user', ''))]
            elif filter_by == 'long':
                # Filter for substantial exchanges
                history = [h for h in history 
                          if len(h.get('user', '')) + len(h.get('assistant', '')) > 200]
            elif filter_by == 'recent':
                # Filter for recent exchanges (last hour)
                cutoff = datetime.now() - timedelta(hours=1)
                history = [h for h in history 
                          if datetime.fromisoformat(h.get('timestamp', datetime.now().isoformat())) > cutoff]
        
        # Limit to last N if specified
        if last_n and last_n > 0:
            history = history[-last_n:]
            
        # Add metadata if requested
        if include_metadata:
            enriched_history = []
            for i, exchange in enumerate(history):
                enriched = exchange.copy()
                enriched['metadata'] = {
                    'index': i,
                    'user_length': len(exchange.get('user', '')),
                    'assistant_length': len(exchange.get('assistant', '')),
                    'is_vision': self.is_vision_command(exchange.get('user', '')),
                    'timestamp_parsed': datetime.fromisoformat(exchange.get('timestamp', datetime.now().isoformat()))
                }
                enriched_history.append(enriched)
            return enriched_history
            
        return history
        
    def set_system_prompt(self, prompt: str, merge_with_default: bool = False):
        """Update system prompt with validation and options"""
        # Validate prompt
        if not prompt or not isinstance(prompt, str):
            logger.warning("Invalid system prompt provided")
            return
            
        # Store original if first time
        if not hasattr(self, '_original_system_prompt'):
            self._original_system_prompt = self.system_prompt
            
        if merge_with_default:
            # Merge with current dynamic prompt
            self._initialize_dynamic_system_prompt(prompt)
        else:
            # Direct replacement but add current context
            current_datetime = datetime.now().strftime("%A, %B %d, %Y at %I:%M %p")
            timezone = self._get_timezone_name()
            
            # Add context to custom prompt
            context_suffix = f"\n\nCurrent date/time: {current_datetime}"
            if timezone:
                context_suffix += f" ({timezone})"
                
            self.system_prompt = prompt + context_suffix
            
        # Track prompt changes
        if not hasattr(self, '_prompt_history'):
            self._prompt_history = []
            
        self._prompt_history.append({
            'timestamp': datetime.now(),
            'prompt': self.system_prompt,
            'merged': merge_with_default
        })
        
        # Limit history
        if len(self._prompt_history) > 20:
            self._prompt_history = self._prompt_history[-20:]
            
        logger.info(f"System prompt updated (merged: {merge_with_default})")
        
    async def get_response(self, prompt: str, **kwargs) -> str:
        """Enhanced alias with additional options and tracking"""
        # Track API usage
        if hasattr(self, '_usage_analytics'):
            self._usage_analytics['total_requests'] += 1
            
        start_time = datetime.now()
        
        try:
            # Check for any kwargs that modify behavior
            if kwargs.get('stream', False):
                # Return streaming generator
                return self.generate_response_stream(prompt, adaptive=kwargs.get('adaptive', True))
                
            # Standard response
            response = await self.generate_response(prompt)
            
            # Track success
            response_time = (datetime.now() - start_time).total_seconds() * 1000  # ms
            if hasattr(self, '_usage_analytics'):
                self._usage_analytics['response_times'].append(response_time)
                # Keep only last 100 response times
                if len(self._usage_analytics['response_times']) > 100:
                    self._usage_analytics['response_times'] = self._usage_analytics['response_times'][-100:]
                # Update average
                self._usage_analytics['avg_response_time'] = sum(self._usage_analytics['response_times']) / len(self._usage_analytics['response_times'])
                
            return response
            
        except Exception as e:
            # Track errors
            if hasattr(self, '_usage_analytics'):
                self._usage_analytics['errors'] += 1
            logger.error(f"Error in get_response: {e}")
            raise
    
    async def _extract_context_summary(self) -> str:
        """Extract important context from conversation history"""
        if not self.conversation_history:
            return ""
            
        # Extract key topics and entities
        topics = set()
        for exchange in self.conversation_history[-5:]:  # Last 5 exchanges
            user_text = exchange.get('user', '').lower()
            # Simple topic extraction
            if 'screen' in user_text or 'vision' in user_text:
                topics.add('vision_analysis')
            if 'error' in user_text or 'bug' in user_text:
                topics.add('troubleshooting')
            if 'code' in user_text:
                topics.add('programming')
                
        return f"Previous topics: {', '.join(topics)}" if topics else ""
    
    def get_health_status(self) -> Dict[str, Any]:
        """Get detailed health and status information"""
        health = {
            'status': 'healthy' if self.is_available() else 'unhealthy',
            'checks': {}
        }
        
        # Detailed checks
        checks = [
            ('api_key_present', bool(self.api_key)),
            ('client_initialized', self.client is not None),
            ('model_valid', self._is_valid_model(self.model) if hasattr(self, '_is_valid_model') else True),
            ('anthropic_library', ANTHROPIC_AVAILABLE),
            ('screenshot_capability', self._check_screenshot_capability() if hasattr(self, '_check_screenshot_capability') else SCREENSHOT_AVAILABLE),
            ('vision_analyzer', self.vision_analyzer is not None),
            ('conversation_history', hasattr(self, 'conversation_history') and isinstance(self.conversation_history, list))
        ]
        
        for check_name, check_result in checks:
            health['checks'][check_name] = check_result
            
        # Overall status
        critical_checks = ['api_key_present', 'client_initialized', 'anthropic_library']
        health['critical_ok'] = all(health['checks'].get(check, False) for check in critical_checks)
        
        return health
    
    async def optimize_for_performance(self):
        """Optimize chatbot for better performance"""
        optimizations_applied = []
        
        # Clear old history if too long
        if len(self.conversation_history) > self.max_history_length * 2:
            await self.clear_history(preserve_last_n=self.max_history_length)
            optimizations_applied.append('trimmed_history')
            
        # Clean up cache
        if hasattr(self, '_screenshot_cache_store'):
            current_time = datetime.now()
            cache_config = getattr(self, '_cache_config', {'duration': timedelta(seconds=5)})
            
            # Remove expired entries
            expired_keys = []
            for key, (timestamp, _, _) in self._screenshot_cache_store.items():
                if current_time - timestamp > cache_config['duration'] * 2:
                    expired_keys.append(key)
                    
            for key in expired_keys:
                del self._screenshot_cache_store[key]
                
            if expired_keys:
                optimizations_applied.append(f'cleared_{len(expired_keys)}_cache_entries')
                
        # Reset analytics if too large
        if hasattr(self, '_usage_analytics') and len(self._usage_analytics.get('response_times', [])) > 1000:
            self._usage_analytics['response_times'] = self._usage_analytics['response_times'][-100:]
            optimizations_applied.append('trimmed_analytics')
            
        # Clear old prompt history
        if hasattr(self, '_prompt_history') and len(self._prompt_history) > 50:
            self._prompt_history = self._prompt_history[-20:]
            optimizations_applied.append('trimmed_prompt_history')
            
        logger.info(f"Performance optimizations applied: {optimizations_applied}")
        return optimizations_applied
    
    async def export_conversation(self, format: str = 'json', include_system: bool = False) -> Union[str, Dict]:
        """Export conversation in various formats"""
        history = await self.get_conversation_history(include_metadata=True)
        
        if format == 'json':
            export_data = {
                'exported_at': datetime.now().isoformat(),
                'model': self.model,
                'conversation_count': len(history),
                'conversations': history
            }
            
            if include_system:
                export_data['system_prompt'] = self.system_prompt
                export_data['configuration'] = {
                    'temperature': self.temperature,
                    'max_tokens': self.max_tokens
                }
                
            return export_data
            
        elif format == 'markdown':
            md_lines = [f"# Conversation Export\n"]
            md_lines.append(f"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            md_lines.append(f"**Model**: {self.model}\n")
            
            if include_system:
                md_lines.append(f"\n## System Prompt\n```\n{self.system_prompt}\n```\n")
                
            md_lines.append(f"\n## Conversation\n")
            
            for i, exchange in enumerate(history):
                timestamp = exchange.get('timestamp', '')
                md_lines.append(f"\n### Exchange {i+1} - {timestamp}\n")
                md_lines.append(f"**User**: {exchange.get('user', '')}\n")
                md_lines.append(f"**Assistant**: {exchange.get('assistant', '')}\n")
                
            return "\n".join(md_lines)
            
        elif format == 'txt':
            txt_lines = [f"Conversation Export - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"]
            txt_lines.append(f"Model: {self.model}")
            txt_lines.append("=" * 50)
            
            for exchange in history:
                txt_lines.append(f"\nUser: {exchange.get('user', '')}")
                txt_lines.append(f"Assistant: {exchange.get('assistant', '')}")
                txt_lines.append("-" * 30)
                
            return "\n".join(txt_lines)
            
        else:
            raise ValueError(f"Unsupported export format: {format}")
    
    def get_capabilities(self) -> Dict[str, Any]:
        """Get detailed capabilities of the chatbot"""
        return {
            'text_generation': True,
            'vision_analysis': True,
            'streaming': True,
            'conversation_memory': True,
            'max_conversation_length': self.max_history_length,
            'supported_image_formats': self._get_supported_image_formats() if hasattr(self, '_get_supported_image_formats') else ['JPEG', 'PNG'],
            'platform_support': {
                'macos': self._platform == 'darwin' if hasattr(self, '_platform') else True,
                'windows': self._platform == 'win32' if hasattr(self, '_platform') else True,
                'linux': 'linux' in self._platform if hasattr(self, '_platform') else True
            },
            'api_features': {
                'models': ['claude-3-opus', 'claude-3-sonnet', 'claude-3-haiku', 'claude-3-5-sonnet'],
                'max_tokens': 4096,
                'vision_enabled': True,
                'temperature_range': (0.0, 1.0)
            },
            'performance_features': {
                'caching': hasattr(self, '_screenshot_cache_store'),
                'adaptive_streaming': True,
                'multi_method_capture': hasattr(self, '_capture_methods'),
                'intent_analysis': hasattr(self, '_analyze_vision_intent')
            }
        }
    
    async def reset(self, keep_config: bool = True):
        """Reset the chatbot to initial state"""
        logger.info(f"Resetting chatbot (keep_config: {keep_config})")
        
        # Clear conversation
        await self.clear_history()
        
        # Reset analytics
        if hasattr(self, '_usage_analytics'):
            self._usage_analytics = {
                'total_requests': 0,
                'vision_requests': 0,
                'errors': 0,
                'total_tokens': 0,
                'cache_hits': 0,
                'cache_misses': 0,
                'avg_response_time': 0,
                'response_times': []
            }
            
        # Clear caches
        if hasattr(self, '_screenshot_cache_store'):
            self._screenshot_cache_store.clear()
            
        if hasattr(self, '_pattern_cache'):
            self._pattern_cache.clear()
            
        # Reset to original prompt
        if hasattr(self, '_original_system_prompt') and not keep_config:
            self.system_prompt = self._original_system_prompt
            
        # Re-initialize if needed
        if not keep_config:
            self.temperature = 0.7
            self.max_tokens = 1024
            
        logger.info("Chatbot reset complete")